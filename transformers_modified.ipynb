{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10270030,"sourceType":"datasetVersion","datasetId":6354147},{"sourceId":10602508,"sourceType":"datasetVersion","datasetId":6562933}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport transformers\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"Keras version:\", keras.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:48:51.985010Z","iopub.execute_input":"2025-03-07T06:48:51.985269Z","iopub.status.idle":"2025-03-07T06:49:09.418477Z","shell.execute_reply.started":"2025-03-07T06:48:51.985245Z","shell.execute_reply":"2025-03-07T06:49:09.417706Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.17.1\nTransformers version: 4.47.0\nKeras version: 3.5.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:12.284739Z","iopub.execute_input":"2025-03-07T06:49:12.285064Z","iopub.status.idle":"2025-03-07T06:49:19.258923Z","shell.execute_reply.started":"2025-03-07T06:49:12.285036Z","shell.execute_reply":"2025-03-07T06:49:19.257987Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/ma-tam/AWM_train.csv')\ndev_df = pd.read_csv('/kaggle/input/ma-tam/AWM_dev.csv')\n# test_without_label = pd.read_csv('/kaggle/input/ma-tam/AWM_test_without_labels.csv')\ntest_with_label = pd.read_csv('/kaggle/input/test-label/AWM_test_with_labels (1).csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:19.260094Z","iopub.execute_input":"2025-03-07T06:49:19.261063Z","iopub.status.idle":"2025-03-07T06:49:19.347195Z","shell.execute_reply.started":"2025-03-07T06:49:19.261028Z","shell.execute_reply":"2025-03-07T06:49:19.346231Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(train_df.shape)\nprint(dev_df.shape)\nprint(test_with_label.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:19.349031Z","iopub.execute_input":"2025-03-07T06:49:19.349416Z","iopub.status.idle":"2025-03-07T06:49:19.354651Z","shell.execute_reply.started":"2025-03-07T06:49:19.349380Z","shell.execute_reply":"2025-03-07T06:49:19.353648Z"}},"outputs":[{"name":"stdout","text":"(2933, 2)\n(629, 2)\n(629, 3)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:19.355619Z","iopub.execute_input":"2025-03-07T06:49:19.355881Z","iopub.status.idle":"2025-03-07T06:49:19.383257Z","shell.execute_reply.started":"2025-03-07T06:49:19.355859Z","shell.execute_reply":"2025-03-07T06:49:19.382250Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                Text        Class\n0  നവ്യയുടെ കയ്യിന്ന് കിട്ടിയതാണല്ലോ. ഇവൾക്ക് അതൊ...      Abusive\n1  \"ഇവരുടെ പ്രശ്നം അസൂയ ആണ്, സുന്ദരികൾ ആയ നടിമാരോ...      Abusive\n2  ചുമ്മാതല്ല ഇവളെ പണ്ട് ലവർ അലക്കി വിട്ടത്...വായ...      Abusive\n3  \"ഒരു സിനിമയിൽ ജഗതി മദാമ്മയായിട്ട് വരുന്നുണ്ടല്...  Non-Abusive\n4  ഈ വർഷത്തെ ബൂലോക തോൽവി പരാജയം അതിനുള്ള ഓസ്‌ക്കാ...      Abusive\n5  ബാക്കിൽ നിക്കുന്ന ചേച്ചി : എന്ത് വെറുപ്പിക്കൽ ...      Abusive\n6  പ്രയാഗ  ശരിക്കും  അമേരിക്കൻ അമ്മായി ലുക്ക് ആയല...  Non-Abusive\n7  പ്രയാഗ ആർന്നോ ഞാൻ വിചാരിച്ചു ഏതോ ഇംഗ്ലീഷ് കാരി...  Non-Abusive\n8  ശെരിക്കും ഇതിന്റെ പാട്ട്  വട്ടാണ് വട്ടാണ് എനിക...  Non-Abusive\n9  നല്ല ഒരു നായിയായിരുന്നു പക്ഷേ ഇപ്പോ ആരും അഭിനയ...      Abusive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>നവ്യയുടെ കയ്യിന്ന് കിട്ടിയതാണല്ലോ. ഇവൾക്ക് അതൊ...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"ഇവരുടെ പ്രശ്നം അസൂയ ആണ്, സുന്ദരികൾ ആയ നടിമാരോ...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ചുമ്മാതല്ല ഇവളെ പണ്ട് ലവർ അലക്കി വിട്ടത്...വായ...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"ഒരു സിനിമയിൽ ജഗതി മദാമ്മയായിട്ട് വരുന്നുണ്ടല്...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ഈ വർഷത്തെ ബൂലോക തോൽവി പരാജയം അതിനുള്ള ഓസ്‌ക്കാ...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ബാക്കിൽ നിക്കുന്ന ചേച്ചി : എന്ത് വെറുപ്പിക്കൽ ...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>പ്രയാഗ  ശരിക്കും  അമേരിക്കൻ അമ്മായി ലുക്ക് ആയല...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>പ്രയാഗ ആർന്നോ ഞാൻ വിചാരിച്ചു ഏതോ ഇംഗ്ലീഷ് കാരി...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ശെരിക്കും ഇതിന്റെ പാട്ട്  വട്ടാണ് വട്ടാണ് എനിക...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>നല്ല ഒരു നായിയായിരുന്നു പക്ഷേ ഇപ്പോ ആരും അഭിനയ...</td>\n      <td>Abusive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dev_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:19.384204Z","iopub.execute_input":"2025-03-07T06:49:19.384480Z","iopub.status.idle":"2025-03-07T06:49:19.392148Z","shell.execute_reply.started":"2025-03-07T06:49:19.384453Z","shell.execute_reply":"2025-03-07T06:49:19.391210Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                Text        Class\n0  ഞാനും എന്റെ ഫാമിലി മോളെ വ്യക്തിത്വവും ഗെയിം കണ...  Non-Abusive\n1  സാരമില്ല മോളെ നീ ആരുടെ മുന്നിലും കരയരുത്  അത് ...  Non-Abusive\n2  ഡോക്ടർക്ക് ഇങ്ങനെ തന്നെ വേണം.  ഈ വിവരമില്ലാത്ത...      Abusive\n3  \"ട്രോളന്മാർക്കും, യൂട്യൂബ് ചാനൽസ് നും ചാകര ആയി .\"  Non-Abusive\n4  ഇപ്പോ പറഞ്ഞ നോ നേര്തെ പറഞ്ഞഗിൽ ഇത്തരം മോശം കമന...  Non-Abusive\n5  നിന്റെ ചേച്ചിമാർ ഓസിന് വോട്ട് പിടിച്ചപ്പോൾ ഓർക...  Non-Abusive\n6  നൈസ് ആയി ഒരു കാരണത്തിനു കാത്ത് നിൽക്കുക ആയിരുന...      Abusive\n7               അന്തം  ഫാൻസിനു  ഇനി  പിരിഞ്ഞു  പോവാം  Non-Abusive\n8  \"ഡോക്ടർ ഇത്രെയും ചീപ്പ്‌ ആയി പോയല്ലോ,,, ഇവന് ന...  Non-Abusive\n9  നല്ല രീതിയിൽ തീരേണ്ട ഒരു കാര്യമായിരുന്നു. ഇപ്പ...  Non-Abusive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ഞാനും എന്റെ ഫാമിലി മോളെ വ്യക്തിത്വവും ഗെയിം കണ...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>സാരമില്ല മോളെ നീ ആരുടെ മുന്നിലും കരയരുത്  അത് ...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ഡോക്ടർക്ക് ഇങ്ങനെ തന്നെ വേണം.  ഈ വിവരമില്ലാത്ത...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"ട്രോളന്മാർക്കും, യൂട്യൂബ് ചാനൽസ് നും ചാകര ആയി .\"</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ഇപ്പോ പറഞ്ഞ നോ നേര്തെ പറഞ്ഞഗിൽ ഇത്തരം മോശം കമന...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>നിന്റെ ചേച്ചിമാർ ഓസിന് വോട്ട് പിടിച്ചപ്പോൾ ഓർക...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>നൈസ് ആയി ഒരു കാരണത്തിനു കാത്ത് നിൽക്കുക ആയിരുന...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>അന്തം  ഫാൻസിനു  ഇനി  പിരിഞ്ഞു  പോവാം</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>\"ഡോക്ടർ ഇത്രെയും ചീപ്പ്‌ ആയി പോയല്ലോ,,, ഇവന് ന...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>നല്ല രീതിയിൽ തീരേണ്ട ഒരു കാര്യമായിരുന്നു. ഇപ്പ...</td>\n      <td>Non-Abusive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test_with_label.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:21.602428Z","iopub.execute_input":"2025-03-07T06:49:21.602731Z","iopub.status.idle":"2025-03-07T06:49:21.611542Z","shell.execute_reply.started":"2025-03-07T06:49:21.602706Z","shell.execute_reply":"2025-03-07T06:49:21.610530Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   id                                               Text        Class\n0   1  സൂരജ് നിന്റെ ആര് ആണ് ആള് ഇറക്കി പേടിപ്പിക്കുക ആണോ  Non-Abusive\n1   2  എത്ര അലക്കി വെളുപ്പിച്ചാലും നിനക്കു അർഹത ഇല്ലാ...      Abusive\n2   3  50 ലക്ഷം കയ്യിൽ വയ്ക്കാൻ ഒരിക്കലും യോഗ്യത ഇല്ല...  Non-Abusive\n3   4  \"ബിഗ് ബോസ്സിൽ നിങ്ങളുടെ അഭിനയം എന്തായിരുന്നു,മ...      Abusive\n4   5  അത് അങ്ങനെയാ നമ്മുടെ ഉള്ളിൽ നന്മ ഉണ്ടെങ്കിൽ പട...  Non-Abusive\n5   6  ഇനി കൂടുതൽ ഒന്നും പറയാതിരിക്കുന്നതായിരിക്കും ന...  Non-Abusive\n6   7  നിങ്ങളെ ആരും ഇഷ്ടപ്പെടുന്നില്ലാ നിങ്ങളുടെ കാര്...      Abusive\n7   8  ഈ ഞാൻ ഞാൻ ...... ഞാനെന്താണെന്ന് മനസ്സിലായി ......      Abusive\n8   9                       ഇതിൻ്റെ ട്രോൾ കാണാൻ കേറി വരൂ  Non-Abusive\n9  10      പണത്തിന്റെ കാര്യം വന്നപ്പോൾ ദേഷ്യം ഒക്കെ പോയോ  Non-Abusive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>സൂരജ് നിന്റെ ആര് ആണ് ആള് ഇറക്കി പേടിപ്പിക്കുക ആണോ</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>എത്ര അലക്കി വെളുപ്പിച്ചാലും നിനക്കു അർഹത ഇല്ലാ...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>50 ലക്ഷം കയ്യിൽ വയ്ക്കാൻ ഒരിക്കലും യോഗ്യത ഇല്ല...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>\"ബിഗ് ബോസ്സിൽ നിങ്ങളുടെ അഭിനയം എന്തായിരുന്നു,മ...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>അത് അങ്ങനെയാ നമ്മുടെ ഉള്ളിൽ നന്മ ഉണ്ടെങ്കിൽ പട...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>ഇനി കൂടുതൽ ഒന്നും പറയാതിരിക്കുന്നതായിരിക്കും ന...</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>നിങ്ങളെ ആരും ഇഷ്ടപ്പെടുന്നില്ലാ നിങ്ങളുടെ കാര്...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>ഈ ഞാൻ ഞാൻ ...... ഞാനെന്താണെന്ന് മനസ്സിലായി ......</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>ഇതിൻ്റെ ട്രോൾ കാണാൻ കേറി വരൂ</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>പണത്തിന്റെ കാര്യം വന്നപ്പോൾ ദേഷ്യം ഒക്കെ പോയോ</td>\n      <td>Non-Abusive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Get the count of samples in each class for each dataset\ntrain_abusive = train_df[train_df['Class'] == 'Abusive'].shape[0]\ntrain_non_abusive = train_df[train_df['Class'] == 'Non-Abusive'].shape[0]\n\ndev_abusive = dev_df[dev_df['Class'] == 'Abusive'].shape[0]\ndev_non_abusive = dev_df[dev_df['Class'] == 'Non-Abusive'].shape[0]\n\ntest_abusive = test_with_label[test_with_label['Class'] == 'Abusive'].shape[0]\ntest_non_abusive = test_with_label[test_with_label['Class'] == 'Non-Abusive'].shape[0]\n\n# Print the counts for each class in the datasets\nprint(f\"Train dataset - Abusive: {train_abusive}, Non-Abusive: {train_non_abusive}\")\nprint(f\"Validation dataset - Abusive: {dev_abusive}, Non-Abusive: {dev_non_abusive}\")\nprint(f\"Test dataset - Abusive: {test_abusive}, Non-Abusive: {test_non_abusive}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:22.164391Z","iopub.execute_input":"2025-03-07T06:49:22.164710Z","iopub.status.idle":"2025-03-07T06:49:22.179034Z","shell.execute_reply.started":"2025-03-07T06:49:22.164685Z","shell.execute_reply":"2025-03-07T06:49:22.178232Z"}},"outputs":[{"name":"stdout","text":"Train dataset - Abusive: 1531, Non-Abusive: 1402\nValidation dataset - Abusive: 303, Non-Abusive: 326\nTest dataset - Abusive: 323, Non-Abusive: 306\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import re\nfrom bs4 import BeautifulSoup\ndef remove_punctuations(text):\n    punctuations = '''!()-[]{};:'\"“\\’,<>./?@#$%^&*_~—॥'''\n    return ''.join([char for char in text if char not in punctuations])\n\n# Function to replace unwanted strings (URLs, emojis, HTML tags, etc.)\ndef replace_strings(text):\n    # Remove URLs\n    url_pattern = r'https?://\\S+|www\\.\\S+'\n    text = re.sub(url_pattern, ' ', text)\n\n    # Remove emojis\n    emoji_pattern = re.compile(\"[\" \n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"  \n        u\"\\u2000-\\u206F\"          # general punctuations\n        \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r' ', text)\n\n    # Remove HTML tags\n    text = BeautifulSoup(text, 'html.parser').get_text()\n\n    # Normalize spaces\n    text = re.sub(r'\\s+', ' ', text)\n\n    return text\n\n# Function for full preprocessing\ndef preprocessing(text):\n    cleaned_text = remove_punctuations(replace_strings(text))\n    return cleaned_text.lower()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:22.638662Z","iopub.execute_input":"2025-03-07T06:49:22.638960Z","iopub.status.idle":"2025-03-07T06:49:22.882632Z","shell.execute_reply.started":"2025-03-07T06:49:22.638937Z","shell.execute_reply":"2025-03-07T06:49:22.881866Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Apply preprocessing to all datasets\ntrain_df['cleanText'] = train_df['Text'].apply(lambda x: preprocessing(str(x)))\ndev_df['cleanText'] = dev_df['Text'].apply(lambda x: preprocessing(str(x)))\ntest_with_label['cleanText'] = test_with_label['Text'].apply(lambda x: preprocessing(str(x)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:23.139345Z","iopub.execute_input":"2025-03-07T06:49:23.139995Z","iopub.status.idle":"2025-03-07T06:49:23.455659Z","shell.execute_reply.started":"2025-03-07T06:49:23.139968Z","shell.execute_reply":"2025-03-07T06:49:23.454737Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-e98d2e6d74aa>:26: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  text = BeautifulSoup(text, 'html.parser').get_text()\n<ipython-input-9-e98d2e6d74aa>:26: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  text = BeautifulSoup(text, 'html.parser').get_text()\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Combine all text for vocabulary analysis\ntrain_corpus = train_df['cleanText'].sum()\ndev_corpus = dev_df['cleanText'].sum()\ntest_corpus = test_with_label['cleanText'].sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:23.588119Z","iopub.execute_input":"2025-03-07T06:49:23.588445Z","iopub.status.idle":"2025-03-07T06:49:23.643084Z","shell.execute_reply.started":"2025-03-07T06:49:23.588418Z","shell.execute_reply":"2025-03-07T06:49:23.642129Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Vocabulary and OOV analysis\ntrain_vocab = set(train_corpus.split())\ndev_vocab = set(dev_corpus.split())\ntest_vocab = set(test_corpus.split())\n\noov_dev = dev_vocab - train_vocab\noov_test = test_vocab - train_vocab\n\nprint(f\"Number of unique words in training data: {len(train_vocab)}\")\nprint(f\"Number of unique words in development data: {len(dev_vocab)}\")\nprint(f\"Number of unique words in test data: {len(test_vocab)}\")\nprint(f\"OOV words in dev dataset: {len(oov_dev)}\")\nprint(f\"OOV words in test dataset: {len(oov_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:27.286682Z","iopub.execute_input":"2025-03-07T06:49:27.286999Z","iopub.status.idle":"2025-03-07T06:49:27.307211Z","shell.execute_reply.started":"2025-03-07T06:49:27.286975Z","shell.execute_reply":"2025-03-07T06:49:27.306266Z"}},"outputs":[{"name":"stdout","text":"Number of unique words in training data: 15675\nNumber of unique words in development data: 4647\nNumber of unique words in test data: 4586\nOOV words in dev dataset: 2525\nOOV words in test dataset: 2496\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Label encoding for train and dev datasets\ntrain_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0})\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0})\n\nprint(\"Training dataset label counts:\")\nprint(train_df['enc_label'].value_counts())\nprint(\"Development dataset label counts:\")\nprint(dev_df['enc_label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:27.815200Z","iopub.execute_input":"2025-03-07T06:49:27.815495Z","iopub.status.idle":"2025-03-07T06:49:27.834688Z","shell.execute_reply.started":"2025-03-07T06:49:27.815473Z","shell.execute_reply":"2025-03-07T06:49:27.833861Z"}},"outputs":[{"name":"stdout","text":"Training dataset label counts:\nenc_label\n1    1531\n0    1402\nName: count, dtype: int64\nDevelopment dataset label counts:\nenc_label\n0    326\n1    303\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-13-7f70e88776ca>:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0})\n<ipython-input-13-7f70e88776ca>:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0})\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"**Pretrained BERT MODELS By Pytorch**","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:30.937248Z","iopub.execute_input":"2025-03-07T06:49:30.937557Z","iopub.status.idle":"2025-03-07T06:49:35.423522Z","shell.execute_reply.started":"2025-03-07T06:49:30.937535Z","shell.execute_reply":"2025-03-07T06:49:35.422429Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"**Malayalam-bert**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# Set a random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for the selected model\nmodel_name = 'l3cube-pune/malayalam-bert'  # Change model as needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 256\nBATCH_SIZE = 32\nsource_to_idx = {'Abusive': 1, 'Non-Abusive': 0}\n\n# Prepare datasets\ntrain_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ntest_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n\nX_train_text = train_df['cleanText'].tolist()\ny_train_labels = train_df['enc_label'].tolist()\n\nX_dev_text = dev_df['cleanText'].tolist()\ny_dev_labels = dev_df['enc_label'].tolist()\nX_test_text = test_with_label['cleanText'].tolist()\ny_test_labels = test_with_label['enc_label'].tolist()  # Ensure labels are here\n\n# Create dataset objects\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        \n        if self.is_labeled and self.labels is not None:  # Check if labels are provided\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\ntrain_set = NewsDataset(X_train_text, y_train_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = NewsDataset(X_dev_text, y_dev_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = NewsDataset(X_test_text, y_test_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)  # Pass labels here\n\n# Create data loaders\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:35.425066Z","iopub.execute_input":"2025-03-07T06:49:35.425415Z","iopub.status.idle":"2025-03-07T06:49:37.113484Z","shell.execute_reply.started":"2025-03-07T06:49:35.425381Z","shell.execute_reply":"2025-03-07T06:49:37.112134Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b0163050c2417ba714393bca8f80b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f28af47b4af4b74a67e299fba629081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cefb6ae7debf42a59bbb40cee530b15c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6514def2805c45a987e71bc1e166749e"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-15-f267d8d29e70>:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-15-f267d8d29e70>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-15-f267d8d29e70>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  test_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import AutoModel\n# BERT Classifier Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BERTClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(0.5)\n        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize model\nmodel = BERTClassifier(model_name=model_name, num_labels=len(source_to_idx))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:40.471384Z","iopub.execute_input":"2025-03-07T06:49:40.471706Z","iopub.status.idle":"2025-03-07T06:49:47.218930Z","shell.execute_reply.started":"2025-03-07T06:49:40.471679Z","shell.execute_reply":"2025-03-07T06:49:47.218216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b93e500ccfd047619197ccabc7e66189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/951M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3400c9d9fd714b98b694eba0172379de"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertModel were not initialized from the model checkpoint at l3cube-pune/malayalam-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(197285, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.5, inplace=False)\n  (out): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Calculate class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.array([0, 1]),  # Convert classes to a NumPy array\n    y=train_df['enc_label']\n)\n\n# Convert class weights to a tensor\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:47.220048Z","iopub.execute_input":"2025-03-07T06:49:47.220329Z","iopub.status.idle":"2025-03-07T06:49:47.230985Z","shell.execute_reply.started":"2025-03-07T06:49:47.220305Z","shell.execute_reply":"2025-03-07T06:49:47.230124Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(params=model.parameters(), lr=2e-6, weight_decay=1e-4)\nloss_function = nn.CrossEntropyLoss(weight=class_weights)\n# Training loop\nEPOCHS = 15\nfor epoch in range(EPOCHS):\n    model.train()\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n\n    total_train_loss = 0\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for batch in train_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(ids, mask)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train_correct += (predicted == targets).sum().item()\n        total_train_samples += targets.size(0)\n\n        train_bar.set_postfix(loss=loss.item())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = total_train_correct / total_train_samples\n\n    model.eval()\n    total_val_loss = 0\n    total_val_correct = 0\n    total_val_samples = 0\n    dev_bar = tqdm(dev_loader, desc=f\"Epoch {epoch+1} - Validation\")\n\n    with torch.no_grad():\n        for batch in dev_bar:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(ids, mask)\n            loss = loss_function(outputs, targets)\n            total_val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_val_correct += (predicted == targets).sum().item()\n            total_val_samples += targets.size(0)\n\n            dev_bar.set_postfix(loss=loss.item())\n\n    avg_val_loss = total_val_loss / len(dev_loader)\n    val_accuracy = total_val_correct / total_val_samples\n\n    print(f\"Epoch {epoch + 1} | \"\n          f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f} | \"\n          f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n# Evaluate on the test set\nmodel.eval()  # Set the model to evaluation mode\n\nall_preds = []\nall_targets = []\n\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch in test_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        if 'targets' in batch:  # Only collect true labels if they exist\n            all_targets.extend(batch['targets'].cpu().numpy())  # Collect true labels\n\n# Generate the classification report if labels exist\nif len(all_targets) > 0:\n    report = classification_report(all_targets, all_preds, target_names=['Non-Abusive', 'Abusive'])\n    print(\"Classification Report on Test Set:\")\n    print(report)\nelse:\n    print(\"Test set does not contain labels, only predictions are available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:49:47.232408Z","iopub.execute_input":"2025-03-07T06:49:47.232677Z","iopub.status.idle":"2025-03-07T07:23:18.677979Z","shell.execute_reply.started":"2025-03-07T06:49:47.232656Z","shell.execute_reply":"2025-03-07T07:23:18.676961Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 - Training: 100%|██████████| 92/92 [01:53<00:00,  1.23s/it, loss=0.693]\nEpoch 1 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.26it/s, loss=0.695]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Training Loss: 0.6933, Training Accuracy: 0.5220 | Validation Loss: 0.6948, Validation Accuracy: 0.4817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Training: 100%|██████████| 92/92 [02:05<00:00,  1.36s/it, loss=0.685]\nEpoch 2 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.09it/s, loss=0.695]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Training Loss: 0.6931, Training Accuracy: 0.5220 | Validation Loss: 0.6945, Validation Accuracy: 0.4817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.38s/it, loss=0.695]\nEpoch 3 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.09it/s, loss=0.694]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Training Loss: 0.6927, Training Accuracy: 0.5220 | Validation Loss: 0.6938, Validation Accuracy: 0.4817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Training: 100%|██████████| 92/92 [02:06<00:00,  1.38s/it, loss=0.699]\nEpoch 4 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.09it/s, loss=0.688]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Training Loss: 0.6908, Training Accuracy: 0.5244 | Validation Loss: 0.6887, Validation Accuracy: 0.4849\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Training: 100%|██████████| 92/92 [02:06<00:00,  1.38s/it, loss=0.658]\nEpoch 5 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.09it/s, loss=0.657]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Training Loss: 0.6782, Training Accuracy: 0.6553 | Validation Loss: 0.6620, Validation Accuracy: 0.7218\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Training: 100%|██████████| 92/92 [02:06<00:00,  1.38s/it, loss=0.607]\nEpoch 6 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.10it/s, loss=0.63] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Training Loss: 0.6523, Training Accuracy: 0.7368 | Validation Loss: 0.6384, Validation Accuracy: 0.7266\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Training: 100%|██████████| 92/92 [02:06<00:00,  1.37s/it, loss=0.601]\nEpoch 7 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.14it/s, loss=0.659]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Training Loss: 0.6260, Training Accuracy: 0.7627 | Validation Loss: 0.6242, Validation Accuracy: 0.7202\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Training: 100%|██████████| 92/92 [02:04<00:00,  1.35s/it, loss=0.568]\nEpoch 8 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.16it/s, loss=0.649]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Training Loss: 0.6001, Training Accuracy: 0.7879 | Validation Loss: 0.6030, Validation Accuracy: 0.7488\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.566]\nEpoch 9 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.18it/s, loss=0.627]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Training Loss: 0.5699, Training Accuracy: 0.8152 | Validation Loss: 0.5862, Validation Accuracy: 0.7520\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.35s/it, loss=0.671]\nEpoch 10 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.17it/s, loss=0.677]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Training Loss: 0.5476, Training Accuracy: 0.8275 | Validation Loss: 0.5816, Validation Accuracy: 0.7472\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.478]\nEpoch 11 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.17it/s, loss=0.695]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Training Loss: 0.5228, Training Accuracy: 0.8415 | Validation Loss: 0.5738, Validation Accuracy: 0.7440\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.443]\nEpoch 12 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.17it/s, loss=0.627]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Training Loss: 0.4963, Training Accuracy: 0.8582 | Validation Loss: 0.5566, Validation Accuracy: 0.7615\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.592]\nEpoch 13 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.18it/s, loss=0.685]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Training Loss: 0.4756, Training Accuracy: 0.8640 | Validation Loss: 0.5517, Validation Accuracy: 0.7647\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.493]\nEpoch 14 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.18it/s, loss=0.623]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Training Loss: 0.4539, Training Accuracy: 0.8701 | Validation Loss: 0.5553, Validation Accuracy: 0.7520\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.495]\nEpoch 15 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.18it/s, loss=0.715]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Training Loss: 0.4337, Training Accuracy: 0.8790 | Validation Loss: 0.5552, Validation Accuracy: 0.7520\n","output_type":"stream"},{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 20/20 [00:09<00:00,  2.18it/s]","output_type":"stream"},{"name":"stdout","text":"Classification Report on Test Set:\n              precision    recall  f1-score   support\n\n Non-Abusive       0.67      0.78      0.72       306\n     Abusive       0.76      0.64      0.69       323\n\n    accuracy                           0.71       629\n   macro avg       0.71      0.71      0.71       629\nweighted avg       0.72      0.71      0.71       629\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Evaluate on the test set\nmodel.eval()  # Set the model to evaluation mode\n\nall_preds = []\nall_targets = []\n\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch in test_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        if 'targets' in batch:  # Only collect true labels if they exist\n            all_targets.extend(batch['targets'].cpu().numpy())  # Collect true labels\n\n# Generate the classification report if labels exist\nif len(all_targets) > 0:\n    # Print classification report\n    report = classification_report(all_targets, all_preds, target_names=['Non-Abusive', 'Abusive'])\n    print(\"Classification Report on Test Set:\")\n    print(report)\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(all_targets, all_preds)\n\n    # Plot the confusion matrix as a heatmap\n    plt.figure(figsize=(4, 3))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Abusive', 'Abusive'], yticklabels=['Non-Abusive', 'Abusive'])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n\nelse:\n    print(\"Test set does not contain labels, only predictions are available.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T07:23:54.367441Z","iopub.execute_input":"2025-03-07T07:23:54.368169Z","iopub.status.idle":"2025-03-07T07:24:03.337020Z","shell.execute_reply.started":"2025-03-07T07:23:54.368122Z","shell.execute_reply":"2025-03-07T07:24:03.336090Z"}},"outputs":[{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 20/20 [00:08<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classification Report on Test Set:\n              precision    recall  f1-score   support\n\n Non-Abusive       0.67      0.78      0.72       306\n     Abusive       0.76      0.64      0.69       323\n\n    accuracy                           0.71       629\n   macro avg       0.71      0.71      0.71       629\nweighted avg       0.72      0.71      0.71       629\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 400x300 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAXYAAAE8CAYAAADUnZpvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIxklEQVR4nO3dd1gUV9sH4N8uZUE6CgJKUVAEROwNa1SKFTsKihU1oMYuSRTFgsbYTSx5jRAFK4HYFQXFQiwodhEUK2AXpEo53x98bFwXdVdYZtl97vea63LPnJl5ZnnzcDhz5hweY4yBEEKIwuBzHQAhhJDKRYmdEEIUDCV2QghRMJTYCSFEwVBiJ4QQBUOJnRBCFAwldkIIUTCU2AkhRMFQYieEEAVDiZ1ILTk5GS4uLtDT0wOPx0NUVFSlnv/hw4fg8XgICQmp1PNWZ126dEGXLl24DoNUE5TYq6n79+9jwoQJqF+/PjQ0NKCrqwtnZ2esXbsWeXl5Mr22j48Pbty4gSVLlmD79u1o2bKlTK9XlUaNGgUejwddXd1yv8fk5GTweDzweDz8+uuvUp8/LS0NCxYsQGJiYiVES0j5VLkOgEjv0KFDGDx4MAQCAUaOHInGjRvjw4cPOHv2LGbNmoVbt25hy5YtMrl2Xl4e4uPj8dNPP8Hf318m17C0tEReXh7U1NRkcv6vUVVVRW5uLg4cOIAhQ4aI7AsLC4OGhgby8/O/6dxpaWlYuHAhrKys0LRpU4mPO378+DddjygnSuzVTGpqKjw9PWFpaYmYmBiYmpoK9/n5+SElJQWHDh2S2fVfvnwJANDX15fZNXg8HjQ0NGR2/q8RCARwdnbGzp07xRJ7eHg4evXqhYiIiCqJJTc3FzVq1IC6unqVXI8oCEaqlYkTJzIA7Ny5cxLVLywsZEFBQax+/fpMXV2dWVpasoCAAJafny9Sz9LSkvXq1YudOXOGtWrVigkEAlavXj0WGhoqrBMYGMgAiGyWlpaMMcZ8fHyE//5Y2TEfO378OHN2dmZ6enpMS0uLNWzYkAUEBAj3p6amMgBs27ZtIsedPHmSdejQgdWoUYPp6emxvn37stu3b5d7veTkZObj48P09PSYrq4uGzVqFMvJyfnq9+Xj48O0tLRYSEgIEwgE7O3bt8J9Fy9eZABYREQEA8BWrFgh3Pf69Ws2Y8YM1rhxY6alpcV0dHSYm5sbS0xMFNaJjY0V+/4+vs/OnTszBwcHdvnyZdaxY0emqanJpk6dKtzXuXNn4blGjhzJBAKB2P27uLgwfX199uzZs6/eK1Fc1MdezRw4cAD169dH+/btJao/btw4zJ8/H82bN8fq1avRuXNnBAcHw9PTU6xuSkoKBg0ahB49emDlypUwMDDAqFGjcOvWLQDAgAEDsHr1agDAsGHDsH37dqxZs0aq+G/duoXevXujoKAAQUFBWLlyJfr27Ytz58598bgTJ07A1dUVL168wIIFCzB9+nScP38ezs7OePjwoVj9IUOG4P379wgODsaQIUMQEhKChQsXShzngAEDwOPx8PfffwvLwsPD0ahRIzRv3lys/oMHDxAVFYXevXtj1apVmDVrFm7cuIHOnTsjLS0NAGBnZ4egoCAAgK+vL7Zv347t27ejU6dOwvO8fv0a7u7uaNq0KdasWYOuXbuWG9/atWthZGQEHx8fFBcXAwA2b96M48ePY/369TAzM5P4XokC4vo3C5FcZmYmA8D69esnUf3ExEQGgI0bN06kfObMmQwAi4mJEZZZWloyACwuLk5Y9uLFCyYQCNiMGTOEZWWt6Y9bq4xJ3mJfvXo1A8Bevnz52bjLa7E3bdqUGRsbs9evXwvLrl27xvh8Phs5cqTY9caMGSNyzv79+7OaNWt+9pof34eWlhZjjLFBgwaxbt26McYYKy4uZiYmJmzhwoXlfgf5+fmsuLhY7D4EAgELCgoSll26dKncv0YYK22VA2CbNm0qd9/HLXbGGDt27BgDwBYvXswePHjAtLW1mYeHx1fvkSg+arFXI1lZWQAAHR0dieofPnwYADB9+nSR8hkzZgCAWF+8vb09OnbsKPxsZGQEW1tbPHjw4Jtj/lRZ3/w///yDkpISiY5JT09HYmIiRo0aBUNDQ2F5kyZN0KNHD+F9fmzixIkinzt27IjXr18Lv0NJDB8+HKdOnUJGRgZiYmKQkZGB4cOHl1tXIBCAzy/9z6m4uBivX7+GtrY2bG1tceXKFYmvKRAIMHr0aInquri4YMKECQgKCsKAAQOgoaGBzZs3S3wtorgosVcjurq6AID3799LVP/Ro0fg8/mwsbERKTcxMYG+vj4ePXokUm5hYSF2DgMDA7x9+/YbIxY3dOhQODs7Y9y4cahduzY8PT2xZ8+eLyb5sjhtbW3F9tnZ2eHVq1fIyckRKf/0XgwMDABAqnvp2bMndHR0sHv3boSFhaFVq1Zi32WZkpISrF69Gg0aNIBAIECtWrVgZGSE69evIzMzU+Jr1qlTR6oHpb/++isMDQ2RmJiIdevWwdjYWOJjieKixF6N6OrqwszMDDdv3pTqOB6PJ1E9FRWVcsuZBKsnfu4aZf2/ZTQ1NREXF4cTJ05gxIgRuH79OoYOHYoePXqI1a2IitxLGYFAgAEDBiA0NBSRkZGfba0DwNKlSzF9+nR06tQJO3bswLFjxxAdHQ0HBweJ/zIBSr8faVy9ehUvXrwAANy4cUOqY4niosRezfTu3Rv3799HfHz8V+taWlqipKQEycnJIuXPnz/Hu3fvYGlpWWlxGRgY4N27d2Lln/5VAAB8Ph/dunXDqlWrcPv2bSxZsgQxMTGIjY0t99xlcSYlJYntu3v3LmrVqgUtLa2K3cBnDB8+HFevXsX79+/LfeBcZt++fejatSu2bt0KT09PuLi4oHv37mLfiaS/ZCWRk5OD0aNHw97eHr6+vvjll19w6dKlSjs/qb4osVczs2fPhpaWFsaNG4fnz5+L7b9//z7Wrl0LoLQrAYDYyJVVq1YBAHr16lVpcVlbWyMzMxPXr18XlqWnpyMyMlKk3ps3b8SOLXtRp6CgoNxzm5qaomnTpggNDRVJlDdv3sTx48eF9ykLXbt2xaJFi7BhwwaYmJh8tp6KiorYXwN79+7Fs2fPRMrKfgGV90tQWnPmzMHjx48RGhqKVatWwcrKCj4+Pp/9HonyoBeUqhlra2uEh4dj6NChsLOzE3nz9Pz589i7dy9GjRoFAHBycoKPjw+2bNmCd+/eoXPnzrh48SJCQ0Ph4eHx2aF038LT0xNz5sxB//79MWXKFOTm5mLjxo1o2LChyMPDoKAgxMXFoVevXrC0tMSLFy/w+++/o27duujQocNnz79ixQq4u7ujXbt2GDt2LPLy8rB+/Xro6elhwYIFlXYfn+Lz+fj555+/Wq93794ICgrC6NGj0b59e9y4cQNhYWGoX7++SD1ra2vo6+tj06ZN0NHRgZaWFtq0aYN69epJFVdMTAx+//13BAYGCodfbtu2DV26dMG8efPwyy+/SHU+omA4HpVDvtG9e/fY+PHjmZWVFVNXV2c6OjrM2dmZrV+/XuTlo8LCQrZw4UJWr149pqamxszNzb/4gtKnPh1m97nhjoyVvnjUuHFjpq6uzmxtbdmOHTvEhjuePHmS9evXj5mZmTF1dXVmZmbGhg0bxu7duyd2jU+HBJ44cYI5OzszTU1Npqury/r06fPZF5Q+HU65bds2BoClpqZ+9jtlTHS44+d8brjjjBkzmKmpKdPU1GTOzs4sPj6+3GGK//zzD7O3t2eqqqrlvqBUno/Pk5WVxSwtLVnz5s1ZYWGhSL1p06YxPp/P4uPjv3gPRLHxGJPiaRIhhBC5R33shBCiYCixE0KIgqHETgghCoYSOyGEKBhK7IQQomAosRNCiIKhxE4IIQpGId881Wwmm7U4iXx6e2kD1yGQKqRRwawlTX7Iu1o9/7+lkImdEEI+i6f4HRWU2AkhyoVf/pTOioQSOyFEuVTi1MnyihI7IUS5UFcMIYQoGGqxE0KIgqEWOyGEKBhqsRNCiIKhFjshhCgYarETQoiCoRY7IYQoGHpBiRBCFAy12AkhRMHwqY+dEEIUixK02BX/Dgkh5GM8nuSbFIKDg9GqVSvo6OjA2NgYHh4eSEpKEu5/8+YNJk+eDFtbW2hqasLCwgJTpkxBZmbmJ+HxxLZdu3ZJFQu12AkhykVGLfbTp0/Dz88PrVq1QlFREX788Ue4uLjg9u3b0NLSQlpaGtLS0vDrr7/C3t4ejx49wsSJE5GWloZ9+/aJnGvbtm1wc3MTftbX15cqFkrshBDlIqNx7EePHhX5HBISAmNjYyQkJKBTp05o3LgxIiIihPutra2xZMkSeHt7o6ioCKqq/6VjfX19mJiYfHMsctMVU1RUhBMnTmDz5s14//49ACAtLQ3Z2dkcR0YIUSg8vsRbQUEBsrKyRLaCggKJLlPWxWJoaPjFOrq6uiJJHQD8/PxQq1YttG7dGn/++ScYY1Ldolwk9kePHsHR0RH9+vWDn58fXr58CQBYvnw5Zs6cyXF0hBCFIkUfe3BwMPT09ES24ODgr16ipKQEP/zwA5ydndG4ceNy67x69QqLFi2Cr6+vSHlQUBD27NmD6OhoDBw4EN9//z3Wr18v3S0yaX8VyICHhwd0dHSwdetW1KxZE9euXUP9+vVx6tQpjB8/HsnJyVKdj9Y8VS605qlyqfCap+6rJa77Lup7sRa6QCCAQCD44nGTJk3CkSNHcPbsWdStW1dsf1ZWFnr06AFDQ0Ps378fampqnz3X/PnzsW3bNjx58kTiuOWij/3MmTM4f/481NXVRcqtrKzw7NkzjqIihCgkKR6eSpLEP+Xv74+DBw8iLi6u3KT+/v17uLm5QUdHB5GRkV9M6gDQpk0bLFq0CAUFBRLHIheJvaSkBMXFxWLlT58+hY6ODgcREUIUlowenjLGMHnyZERGRuLUqVOoV6+eWJ2srCy4urpCIBBg//790NDQ+Op5ExMTYWBgINUvGLlI7C4uLlizZg22bNkCoHQcZ3Z2NgIDA9GzZ0+OoyOEKBQZDXf08/NDeHg4/vnnH+jo6CAjIwMAoKenB01NTWRlZcHFxQW5ubnYsWOH8GEsABgZGUFFRQUHDhzA8+fP0bZtW2hoaCA6OhpLly6V+lmjXPSxP336FK6urmCMITk5GS1btkRycjJq1aqFuLg4GBsbS3U+6mNXLtTHrlwq3Mfe53eJ6+Yd+F7iurzP/CWwbds2jBo1CqdOnULXrl3LrZOamgorKyscPXoUAQEBSElJAWMMNjY2mDRpEsaPHw8+X/JfSHKR2IHS4Y67du3C9evXkZ2djebNm8PLywuamppSn4sSu3KhxK5cKpzY+26UuG7e/kkVuxhH5KIrJj8/HxoaGvD29uY6FEKIoqO5YqqGsbExfHx8EB0djZKSEq7DIYQoMhnNFSNP5CKxh4aGIjc3F/369UOdOnXwww8/4PLly1yHRQhRRFK8eVpdyUXk/fv3x969e/H8+XMsXboUt2/fRtu2bdGwYUMEBQVxHR4hRIHw+HyJt+pKriLX0dHB6NGjcfz4cVy/fh1aWlpYuHAh12ERQhRIedPifm6rruQqsefn52PPnj3w8PBA8+bN8ebNG8yaNYvrsAghioQnxVZNycWomGPHjiE8PBxRUVFQVVXFoEGDcPz4cXTq1Inr0AghCqY6t8QlJReJvX///ujduzf++usv9OzZ86tzJxBCyLeixF5Fnj9/TnPCEEKqBCV2GcrKyoKuri6A0slzyuZMKE9ZPUIIqShK7DJkYGCA9PR0GBsbQ19fv9wvmzEGHo9X7syPhBDyTRQ/r3OX2GNiYoRLRsXGxnIVBiFEyVCLXYY6d+5c7r8JIUSWlCGxy8U49qNHj+Ls2bPCz7/99huaNm2K4cOH4+3btxxGRghRNHw+X+KtupKLyGfNmiV8eHrjxg1Mnz4dPXv2RGpqKqZPn85xdIQQhUIvKFWN1NRU2NvbAwAiIiLQp08fLF26FFeuXKEVlAghlYq6YqqIuro6cnNzAQAnTpyAi4sLAMDQ0PCLwyAJIURayjBXjFy02Dt06IDp06fD2dkZFy9exO7duwEA9+7dK3eVb0II+VbVOWFLSi5a7Bs2bICqqir27duHjRs3ok6dOgCAI0eOwM3NjePoCCEKhfrYq4aFhQUOHjwoVr569WoOoiGEKDJlaLHLRWJ//PjxF/dbWFhUUSSEEEVHib2KWFlZffHLpikFCCGVRRkSu1z0sV+9ehVXrlwRbhcuXMCmTZvQsGFD7N27l+vwCCEKhMfnSbxJIzg4GK1atYKOjg6MjY3h4eGBpKQkkTr5+fnw8/NDzZo1oa2tjYEDB+L58+cidR4/foxevXqhRo0aMDY2xqxZs1BUVCRVLHLRYndychIra9myJczMzLBixQoMGDCAg6gIIYpIVi3206dPw8/PD61atUJRURF+/PFHuLi44Pbt29DS0gIATJs2DYcOHcLevXuhp6cHf39/DBgwAOfOnQNQ2jvRq1cvmJiY4Pz580hPT8fIkSOhpqaGpUuXSn6PjDEmk7usBCkpKXByckJOTo5Ux2k285dRREQevb20gesQSBXSqGBztO73URLXffq7xzdf5+XLlzA2Nsbp06fRqVMnZGZmwsjICOHh4Rg0aBAA4O7du7Czs0N8fDzatm2LI0eOoHfv3khLS0Pt2rUBAJs2bcKcOXPw8uVLqKurS3RtueiKycrKEtkyMzNx9+5d/Pzzz2jQoAHX4RFCFIg0LygVFBSI5aeCggKJrpOZmQkAwllsExISUFhYiO7duwvrNGrUCBYWFoiPjwcAxMfHw9HRUZjUAcDV1RVZWVm4deuWxPcoF10x5c3HzhiDubk5du3axVFU3Js5xgUe3zmhoVVt5BUU4sK1B/hp7T9IfvRCWGf9T574ro0tTI30kJ1XgH+vpeLntf/g3sP/+u26tG6IwO97w8HGDDl5HxB24AICfzuA4uISLm6LSOH58+dYs2oFzp05g/z8PJhbWCJo8VI4NHYEADg52JZ73LQZszBqzLiqDLX6kKInJjg4GAsXLhQpCwwMxIIFC754XElJCX744Qc4OzujcePGAICMjAyoq6tDX19fpG7t2rWRkZEhrPNxUi/bX7ZPUnKR2D+dj53P58PIyAg2NjZQVZWLEDnRsbkNNu2OQ8KtR1BVVcFC/z44uNEfzQYsRm7+BwDA1TtPsOvIJTxJfwtDvRr4aWIvHPzdD416B6KkhMGxYR1ErZ+E5VuPYey8v2BmrI/1P3pCRYWPgNWRHN8h+ZKszEyM8h6Glq3b4LdNf8DA0ACPHz2Crq6esM7JU2dFjjl7Ng4L5v2E7j1cqzrcakOaPvaAgACxiQgFAsFXj/Pz88PNmzdFZq2tSnKRNWk+9vL18/9d5LNv4A48iVmGZvbmOHflPgDgz7/PCfc/Tn+Dhb8dwKU9P8LSrCZSn77CIJfmuJmchuAtRwEAD568wk9ro7Bj+Rgs2XwY2bmS/VlJqt6fW/9AbRMTLFoSLCyrW9dcpE4tIyORz6diTqJV6zaoay5aj/xHmsQuEAgkSuQf8/f3x8GDBxEXFycyJYqJiQk+fPiAd+/eibTanz9/DhMTE2GdixcvipyvbNRMWR1JyEUfOwAkJSXB398f3bp1Q7du3eDv74+7d+9yHZZc0dXWAAC8zcwtd38NDXWM7NsWqU9f4WlG6Tz2AnVV5BcUitTLKyiEpoY6mtnRi1/y7HRsDBwcGmPmtCno0rEdhgz0QMTePZ+t//rVK5yJO43+AwZVYZTVj6wmAWOMwd/fH5GRkYiJiUG9evVE9rdo0QJqamo4efKksCwpKQmPHz9Gu3btAADt2rXDjRs38OLFf92t0dHR0NXVFc6AKwm5SOwRERFo3LgxEhIS4OTkBCcnJ1y5cgWOjo6IiIj44rHlPdxgJYr3QhOPx8OKmYNw/up93L6fLrLPd3BHvDy3Eq/jV8HF2R69Jm1AYVHpdxB9/g7aOtXHELcW4PN5MDPSw4++7gAAUyNaJFyePX36BHt274SFpRU2btmKIUOHYXnwYuyPKr8Lbf8/kahRQwvderhUcaTVi6wSu5+fH3bs2IHw8HDo6OggIyMDGRkZyMvLAwDo6elh7NixmD59OmJjY5GQkIDRo0ejXbt2aNu2LQDAxcUF9vb2GDFiBK5du4Zjx47h559/hp+fn1R/OcjFcEdra2t4eXkhKChIpDwwMBA7duzA/fv3P3vsggULxB5uqNRuBTXT1jKJlStrfxwKV2d7dBu9Gs9evBPZp6utASNDHZjU0sUPI7vDzEgP341ehYIPpS81TPH+Dj/6ukNLUx0FhUVY9sdRLJrSDyPm/Il9x69wcDeVS1GHO7ZwagyHxo3xV9h/AwiWLV2MWzdvYHv4brH6/Xq7oW07ZwT8NK8qw6xyFR3uWH/6YYnrPlgl+XoQn/tFsG3bNowaNQpA6QtKM2bMwM6dO1FQUABXV1f8/vvvIt0sjx49wqRJk3Dq1CloaWnBx8cHy5Ytk+p5o1wk9ho1auD69euwsbERKU9OToaTk5NwrvbyFBQUiA0/Mu44Bzy+ikxi5cLqOYPRu0sTdB+7Bo/SXn+xrpqqCtLjfsH3QeHYczRBZJ+pkR7eZuXC0swQiX/PQwevX5Bw+8vz9FQHiprY3bp3Rdv27bEgaImwbM+ucGzZvBEnYs+I1L2ScBmjR3phT8Q/sG3UqKpDrVIVTezWM45IXPf+SveKXYwjcvHwtEuXLjhz5oxYYj979iw6duz4xWPLe7ihaEm973dOcBm/9qtJHfj/PzPBg7qa+I82/WXpuNohbi3xJP0Nrt59UunxksrTtFlzPExNFSl79PAhzMzqiNWNjNgHewcHhU/qlUEJporhLrHv379f+O++fftizpw5SEhIEPY1/fvvv9i7d69YN4syWRMwBEPdW2LwtC3IzslH7Zo6AIDM7HzkFxTCqk5NDHJtgZPxd/DqbTbq1NbHjNEuyCsoxLGz/73MMG1kNxw/fwclJSXo160pZo7uAe/Zf6KkhPM/1sgXeI/0gY/3MPxvyya4uLrj5o3r2LdvD+YvEO2yzM7OxvHjRzFj1hyOIq1elGESMM66YiRdAZzH40k9u6OiTCmQd7X8Lobx87djx4ELMDXSw+/zh6OZnTkMdGvgxev3OHslBUu3HBF5ienI5sloamcOgZoqbtx7hiVbjuD4udtVdRsyp6hdMQBw+lQs1q1ZhcePHqJO3boYMXI0Bg4eIlJn357dWLF8KU6cOgsdHR2OIq06Fe2KaTj7qMR17/1SPRf6kYs+9sqmKImdSEaREzsRV9HEbjvnmMR1k5ZXzxe95GK44+e8e/cOGzbQf7SEkMrD40m+VVdymdhPnjyJ4cOHw9TUFIGBgVyHQwhRIHw+T+KtupKbxP7kyRMEBQWhXr16cHFxAY/HQ2RkpFQT3xBCyNdQi13GCgsLsXfvXri6usLW1haJiYlYsWIF+Hw+fvrpJ7i5uUFNTY3LEAkhCkYZWuycjmOvU6cOGjVqBG9vb+zatQsGBgYAgGHDhnEZFiFEgSnDcEdOE3tRUZFwTgYVFcV5qYgQIr+UIbFz2hWTlpYGX19f7Ny5EyYmJhg4cCAiIyOV4osnhHCD+thlTENDA15eXoiJicGNGzdgZ2eHKVOmoKioCEuWLEF0dLTULycRQsiXyGp2R3kiN6NirK2tsXjxYjx69AgHDx5EQUEBevfuLbZMFCGEVIQytNjlYhKwj/H5fPTs2RM9e/bEy5cvsX37dq5DIoQokOrcEpeU3LTYyzg6OuLJk9JZB42MjMTWGySEkIqgFjsHHj58iMLCwq9XJISQb6AMLXa5S+yEECJLSpDX5S+xd+zYEZqamlyHQQhRUNX5jVJJyV1iP3xY8vUICSFEWtQVU4WSk5MRGxuLFy9eoKSkRGTf/PnzOYqKEKJolCCvy0di/+OPPzBp0iTUqlULJiYmIr9ReTweJXZCSKWhFnsVWbx4MZYsWYI5c2jNRkKIbClBXpePxP727VsMHjyY6zAIIUpAGVrscvGC0uDBg3H8+HGuwyCEKAFZzhUTFxeHPn36wMzMDDweD1FRURJde8WKFcI6VlZWYvuXLVsmVRxy0WK3sbHBvHnz8O+//8LR0VFscY0pU6ZwFBkhRNHIssGek5MDJycnjBkzBgMGDBDbn56eLvL5yJEjGDt2LAYOHChSHhQUhPHjxws/6+joSBWHXCT2LVu2QFtbG6dPn8bp06dF9vF4PErshJBKI8uuGHd3d7i7u392v4mJicjnf/75B127dkX9+vVFynV0dMTqSkMuEntqairXIRBClIQ0LygVFBSgoKBApEwgEEAgEFQ4jufPn+PQoUMIDQ0V27ds2TIsWrQIFhYWGD58OKZNmwZVVcnTtVz0sX+MMQbGGNdhEEIUlDSTgAUHB0NPT09kCw4OrpQ4QkNDoaOjI9ZlM2XKFOzatQuxsbGYMGECli5ditmzZ0t1brlosQPAX3/9hRUrViA5ORkA0LBhQ8yaNQsjRozgODJCiCLhS9EVExAQIDbDbGW01gHgzz//hJeXFzQ0NETKP75ekyZNoK6ujgkTJiA4OFjia8tFYl+1ahXmzZsHf39/ODs7AwDOnj2LiRMn4tWrV5g2bRrHERJCFIU0XeyV1e3yqTNnziApKQm7d+/+at02bdqgqKgIDx8+hK2trUTnl4vEvn79emzcuBEjR44UlvXt2xcODg5YsGABJXZCSKWRh3HsW7duRYsWLeDk5PTVuomJieDz+TA2Npb4/HKR2NPT09G+fXux8vbt24sNDyKEkIqQ5eSO2dnZSElJEX5OTU1FYmIiDA0NYWFhAQDIysrC3r17sXLlSrHj4+PjceHCBXTt2hU6OjqIj4/HtGnT4O3tDQMDA4njkIuHpzY2NtizZ49Y+e7du9GgQQMOIiKEKCpZvqB0+fJlNGvWDM2aNQNQ2l/erFkzkfmudu3aBcYYhg0bJna8QCDArl270LlzZzg4OGDJkiWYNm0atmzZIt09MjkYghIREYGhQ4eie/fuwj72c+fO4eTJk9izZw/69+8v1fk0m/nLIkwip95e2sB1CKQKaVSwn6HX5osS1z00oXXFLsYRueiKGThwIC5cuIBVq1YJX8G1s7PDxYsXhb/5CCGkMvDAfR+7rMlFYgeAFi1aICwsjOswCCEKToVWUJItPp//1X4sHo+HoqKiKoqIEKLo5GBQjMxxmtgjIyM/uy8+Ph7r1q0TW02JEEIqQpoXlKorThN7v379xMqSkpIwd+5cHDhwAF5eXggKCuIgMkKIolKCvC4fwx0BIC0tDePHj4ejoyOKioqQmJiI0NBQWFpach0aIUSByHK4o7zgPLFnZmZizpw5sLGxwa1bt3Dy5EkcOHAAjRs35jo0QogCkmYSsOqK066YX375BcuXL4eJiQl27txZbtcMIYRUJupjl7G5c+dCU1MTNjY2CA0NLXdeYgD4+++/qzgyQoiiUvy0znFiHzlyZLXuxyKEVD/KkHO+KbGfOXMGmzdvxv3797Fv3z7UqVMH27dvR7169dChQweJzxMSEvItlyeEkG+mDC8oSf3wNCIiAq6urtDU1MTVq1eFy0ZlZmZi6dKllR4gIYRUJmV4eCp1Yl+8eDE2bdqEP/74A2pqasJyZ2dnXLlypVKDI4SQyqYMwx2l7opJSkpCp06dxMr19PTw7t27yoiJEEJkRgl6YqRvsZuYmIhMJF/m7NmzqF+/fqUERQghsqIMLXapE/v48eMxdepUXLhwATweD2lpaQgLC8PMmTMxadIkWcRICCGVhifFVl1J3RUzd+5clJSUoFu3bsjNzUWnTp0gEAgwc+ZMTJ48WRYxEkJIpaEXlMrB4/Hw008/YdasWUhJSUF2djbs7e2hra0ti/gIIaRSKUFe//YXlNTV1WFvb1+ZsRBCiMxV575zSUmd2Lt27frFLyYmJqZCARFCiCwpQV6XPrE3bdpU5HNhYSESExNx8+ZN+Pj4VFZchBAiE8rw5qnUiX316tXlli9YsADZ2dkVDogQQmRJGbpieIwxVhknSklJQevWrfHmzZvKOF2FnLz7iusQSBXyXH6S6xBIFXq5bWiFjp8ceUfiuuv720l17ri4OKxYsQIJCQlIT09HZGQkPDw8hPtHjRolNoutq6srjh49Kvz85s0bTJ48GQcOHACfz8fAgQOxdu1aqQaoVNpCG/Hx8dDQ0Kis0xFCiEzI8gWlnJwcODk54bfffvtsHTc3N6Snpwu3nTt3iuz38vLCrVu3EB0djYMHDyIuLg6+vr5SxSF1V8yAAQNEPjPGkJ6ejsuXL2PevHnSno4QQqqULLvY3d3d4e7u/sU6AoEAJiYm5e67c+cOjh49ikuXLqFly5YAgPXr16Nnz5749ddfYWZmJlEcUrfY9fT0RDZDQ0N06dIFhw8fRmBgoLSnI4SQKsXnSb4VFBQgKytLZCub0fZbnTp1CsbGxrC1tcWkSZPw+vVr4b74+Hjo6+sLkzoAdO/eHXw+HxcuXJD4GlK12IuLizF69Gg4OjrCwMBAmkMJIUQuSNPFEhwcjIULF4qUBQYGYsGCBd90bTc3NwwYMAD16tXD/fv38eOPP8Ld3R3x8fFQUVFBRkYGjI2NRY5RVVWFoaEhMjIyJL6OVIldRUUFLi4uuHPnDiV2Qki1JE1XTEBAAKZPny5SJhAIvvnanp6ewn87OjqiSZMmsLa2xqlTp9CtW7dvPu+npO6Kady4MR48eFBpARBCSFWSZqENgUAAXV1dka0iif1T9evXR61atYQz5pqYmODFixcidYqKivDmzZvP9suX55sW2pg5cyYOHjyI9PR0sf4nQgiRZ6o8nsSbrD19+hSvX7+GqakpAKBdu3Z49+4dEhIShHViYmJQUlKCNm3aSHxeibtigoKCMGPGDPTs2RMA0LdvX5G+KsYYeDweiouLJb44IYRUNVnm6+zsbJH1KlJTU5GYmAhDQ0MYGhpi4cKFGDhwIExMTHD//n3Mnj0bNjY2cHV1BQDY2dnBzc0N48ePx6ZNm1BYWAh/f394enpKPCIGkCKxL1y4EBMnTkRsbKwUt0kIIfJFltP2Xr58GV27dhV+Luuf9/HxwcaNG3H9+nWEhobi3bt3MDMzg4uLCxYtWiTSvRMWFgZ/f39069ZN+ILSunXrpIpD4sRe9oJq586dpboAIYTIE1m22Lt06YIvvcx/7Nixr57D0NAQ4eHhFYpDqlExyjDHAiFEsSnBHGDSJfaGDRt+NbnLw1wxhBDyObSC0icWLlwIPT09WcVCCCEypwR5XbrE7unpKfZWFCGEVCfUFfMR6l8nhCgCHhQ/l0k9KoYQQqoz1UqbrFx+SZzYS0pKZBkHIYRUCWXofZB6PnZCCKnOqI+dEEIUjBI02CmxE0KUC41jJ4QQBUNdMYQQomCUoMFOiZ0Qolz4NI6dEEIUC7XYCSFEwagqQSc7JXZCiFKhFjshhCgYGu5ICCEKRgnyOiV2QohyUYI5wCixE0KUC00CRgghCkbx0zoldkKIklGGh6dy09304cMHJCUloaioiOtQCCEKjCfFVl1xnthzc3MxduxY1KhRAw4ODnj8+DEAYPLkyVi2bBnH0RFCFA2PJ/kmrbi4OPTp0wdmZmbg8XiIiooS7issLMScOXPg6OgILS0tmJmZYeTIkUhLSxM5h5WVFXg8nsgmbS7kPLEHBATg2rVrOHXqFDQ0NITl3bt3x+7duzmMjBCiiFR4PIk3aeXk5MDJyQm//fab2L7c3FxcuXIF8+bNw5UrV/D3338jKSkJffv2FasbFBSE9PR04TZ58mSp4uC8jz0qKgq7d+9G27ZtRZ5WOzg44P79+xxGRghRRLIcFePu7g53d/dy9+np6SE6OlqkbMOGDWjdujUeP34MCwsLYbmOjg5MTEy+OQ7OW+wvX76EsbGxWHlOTo5SDEsihFQtafrYCwoKkJWVJbIVFBRUWiyZmZng8XjQ19cXKV+2bBlq1qyJZs2aYcWKFVI/e+Q8sbds2RKHDh0Sfi5L5v/73//Qrl07rsIihCioT/uvv7QFBwdDT09PZAsODq6UOPLz8zFnzhwMGzYMurq6wvIpU6Zg165diI2NxYQJE7B06VLMnj1bqnNz3hWzdOlSuLu74/bt2ygqKsLatWtx+/ZtnD9/HqdPn+Y6PEKIgpGmNRsQEIDp06eLlAkEggrHUFhYiCFDhoAxho0bN4rs+/h6TZo0gbq6OiZMmIDg4GCJr815i71Dhw5ITExEUVERHB0dcfz4cRgbGyM+Ph4tWrTgOjxCiIKRpsUuEAigq6srslU0sZcl9UePHiE6OlqktV6eNm3aoKioCA8fPpT4Gpy32AHA2toaf/zxB9dhEEKUAJdP7sqSenJyMmJjY1GzZs2vHpOYmAg+n1/us8jP4Tyxd+/eHd7e3hgwYMBXf3MRQkhFyXJMRnZ2NlJSUoSfU1NTkZiYCENDQ5iammLQoEG4cuUKDh48iOLiYmRkZAAADA0Noa6ujvj4eFy4cAFdu3aFjo4O4uPjMW3aNHh7e8PAwEDiODjvinFwcEBAQABMTEwwePBg/PPPPygsLOQ6LEKIguKDJ/EmrcuXL6NZs2Zo1qwZgNL+8mbNmmH+/Pl49uwZ9u/fj6dPn6Jp06YwNTUVbufPnwdQ2n+/a9cudO7cGQ4ODliyZAmmTZuGLVu2SBUHjzHGpI6+kpWUlODEiRMIDw9HZGQkVFRUMGjQIHh5eaFz585Sn+/k3VcyiJLIK8/lJ7kOgVShl9uGVuj4QzdfSFy3V2PJuz/kCectdgDg8/lwcXFBSEgInj9/js2bN+PixYv47rvvuA6NEKJgZDmlgLzgvI/9YxkZGdi1axd27NiB69evo3Xr1lyHRAhRMN/SxVLdcN5iz8rKwrZt29CjRw+Ym5tj48aN6Nu3L5KTk/Hvv/9yHR4hRMFQi70K1K5dGwYGBhg6dCiCg4PRsmVLrkMihCiw6pywJcV5Yt+/fz+6desGPp/zPx4IIUqApwRdMZwn9h49enAdAiFEifAVP69zk9ibN2+OkydPwsDAAM2aNfviLI5XrlypwsgIIYqOWuwy0q9fP+F8Cx4eHlyEQAhRUtTHLiOBgYHl/puIS76ViOjIcDxJuYvMt6/hGxCMpm07CfdfjT+FM0ej8OR+EnLeZyFg9TaY128odp4Hd29i/47NeHjvNvh8PurWawD/BauhXgkz1ZHKMbWXHXq1qIsGJjrIKyzGpZRXCNp7Hfcz3gvrCFT5CPJsCo82FhCo8hF7MwOztyfgZVbpHOGezlZYP65Nuee3mxKFV+8rby7x6upbVkaqbjjvY3/y5Al4PB7q1q0LALh48SLCw8Nhb28PX19fjqPj3of8PNS1skH7br2wZdmP5ezPh41dE7Rw/g5hvy0v9xwP7t7EhoXT4TpwBIb4ToMKXwVPH6aApwydjdVIe1sj/HkyGVdT30BVhY+fBjpi74zO6PDTEeR+KAYALBrWDD2cTDH29/PIyi3EMu/mCPHvgF5LS9++jbr4BDE3MkTOu35cawjUVCip/z/qiqkCw4cPh6+vL0aMGIGMjAx0794djRs3RlhYGDIyMjB//nyuQ+SUQ4t2cGjx+QVH2nR1AwC8fp7+2Tr7tq5F196D4DpohLCsdl3LyguSVIqhq+JEPk/eehF313nAycoQ8fdeQkdTDV6d6mHi5n9x9k7pa/FTtl5EfHBPtKhfEwkPXiO/sBj5hcXCc9TUEaCDnTF++PNSld6LPFOCBjv3LyjdvHlT+Ibpnj174OjoiPPnzyMsLAwhISHcBqcA3r97i4f3bkNbzwArZk/AnJG9sepHP6TcvsZ1aOQrdDXVAABvcz4AAJysDKCuqoLTt54L66RkvMeTVzloaVP+9K9D2lsh70MxDlx+KvuAqwlplsarrjhP7IWFhcIHqSdOnBCu2N2oUSOkp3++FVqmvDUJP3ygPznLvHr+DABweNef6ODSF/4LVsGifkOsmzcVL9KecBwd+RweD1g8rBku3HuJu88yAQDGehooKCxGVp7o7Kcvs/JhrKdR7nm8OtZDxL+PRVrxyo7P40m8VVecJ3YHBwds2rQJZ86cQXR0NNzcSrsW0tLSJJqEvrw1CXduWSvrsKuNkpLSyTs7uPZDu+69YF6/IQaNmwrjOhY4f+Igx9GRz1nu3QKN6uph/Kb4bz5HS+uasK2jh7C4B5UYWfVHLfYqsHz5cmzevBldunTBsGHD4OTkBKD0jVRJJgELCAhAZmamyDbMd6qsw6429AxLfzmamNcTKTepa4m3L5+Xdwjh2DLv5nBpaob+y2OR/jZPWP4iMx8CNRVhF00ZI10NvMjMFzuPd6f6uPHoLa4/eivzmKsVJcjsnD887dKlC169eoWsrCyRFUJ8fX1Ro0aNrx4vEAjE1iBUV/9Q6XFWVzWNTaFnWAsvnj0SKX+R9gQOLdpyFBX5nGXezdGzeR14LI/F41c5IvuuPXyLD0XF6GRfGwcTSvvMrU10YF5LC5dTXovU1RKool8rcyyOuF5lsVcXNCqmiqioqIgt+2RlZcVNMHImPy8XL9P/e/D1+nkanjy4By0dXRgamSDnfRbevMxA5pvSxUWeP3sMANA1qAk9g5rg8Xjo0X84Du7cijpWDVC3fgNciDmM588eYfycxZzcEynf8hEtMLCtBUauO4vsvCIY65b2m2flFSK/sBjv8woRFpeKIM+meJvzAe/zChHs3RwXU14h4YFoYvdobQ4VFR72nn9U3qWUWjXuOpcY54m9Xr16X5xS4MED5e4ffJxyF2t+niz8HPHnegBA2+/cMXLqz7h+8Qy2r1sq3P/nr6UvfPX0HIPew8YCAL7rOxSFHz5g39Z1yM3OQh0rG0xeuAZGpnWr8E7I14z5zgYA8M9c0QVmJv/vAnadewgAmLfzKhhj2ObXHupqKoi9mYE5fyWInWt4p/o4lPBM7EErUY7EzvnSeGvXij7oLCwsxNWrV3H06FHMmjULc+fOlfqctDSecqGl8ZRLRZfGu5yaJXHdlvV0K3QtrnDeYp86tfwHnb/99hsuX75cxdEQQhSdMrTYOR8V8znu7u6IiIjgOgxCiIJRgkEx3LfYP2ffvn0wNDTkOgxCiKKpzhlbQpwn9k/nY2eMISMjAy9fvsTvv//OYWSEEEWkDMMdOe+K8fDwQL9+/YTbgAEDEBgYiJs3b9LsjoSQSifLxazj4uLQp08fmJmZgcfjISoqSmQ/Ywzz58+HqakpNDU10b17dyQnJ4vUefPmDby8vKCrqwt9fX2MHTsW2dnZUsXBeYud5mMnhFQlWbbXc3Jy4OTkhDFjxmDAgAFi+3/55ResW7cOoaGhqFevHubNmwdXV1fcvn0bGhql7y14eXkhPT0d0dHRKCwsxOjRo+Hr64vw8HCJ4+B8uCMAFBcXIzIyEnfu3AEA2Nvbo1+/flBV/bbfOzTcUbnQcEflUtHhjteevP96pf/XyFgdBQWikwqW97Z7eXg8HiIjI4WrxDHGYGZmhhkzZmDmzJkAgMzMTNSuXRshISHw9PTEnTt3YG9vj0uXLqFly5YAgKNHj6Jnz554+vQpzMzMJIqb866YW7duoUGDBvDx8UFkZCQiIyPh4+ODBg0a4ObNm1yHRwhRMDwp/lfeJIPBwcHfdN3U1FThmhNl9PT00KZNG8THl072Fh8fD319fWFSB4Du3buDz+fjwoULEl+L866YcePGoXHjxkhISBBOK/D27VuMGjUKvr6+OH/+PMcREkIUiTQLhwUEBGD69OkiZZK01suTkVG6slXt2rVFymvXri3cl5GRAWNjY5H9qqqqMDQ0FNaRBOeJPTExEZcvXxaZK8bAwABLlixBq1atOIyMEKKQpEjskna7yBvOu2IaNmyI58/Fp4998eIFbGxsOIiIEKLIpOmKqUwmJiYAIJbvnj9/LtxnYmKCFy9eiOwvKirCmzdvhHUkwUli/3i1o+DgYEyZMgX79u3D06dP8fTpU+zbtw8//PADli8vf3FmQgj5VrIc7vgl9erVg4mJCU6e/O9hf1ZWFi5cuIB27UrXNW7Xrh3evXuHhIT/JnaLiYlBSUkJ2rRpI/G1OOmK0dfXF3spaciQIcKysoE6ffr0QXExLelFCKk8shzumJ2djZSUFOHn1NRUJCYmwtDQEBYWFvjhhx+wePFiNGjQQDjc0czMTDhyxs7ODm5ubhg/fjw2bdqEwsJC+Pv7w9PTU+IRMQBHiT02Nlaiejdu3JBxJIQQpSPDzH758mV07dpV+LnswauPjw9CQkIwe/Zs5OTkwNfXF+/evUOHDh1w9OhR4Rh2AAgLC4O/vz+6desGPp+PgQMHYt26dVLFIRfj2D/2/v177Ny5E//73/+QkJDwTS12GseuXGgcu3Kp6Dj2u+m5EtdtZPr1VdzkEecPT8vExcXBx8cHpqam+PXXX/Hdd9/h33//5TosQoiC4aqPvSpxOtwxIyMDISEh2Lp1K7KysjBkyBAUFBQgKioK9vb2XIZGCFFQ1ThfS4yzFnufPn1ga2uL69evY82aNUhLS8P69eu5CocQoiR4PJ7EW3XFWYv9yJEjmDJlCiZNmoQGDRpwFQYhRMlU43wtMc5a7GfPnsX79+/RokULtGnTBhs2bMCrV/TQkxAiW8qwghJnib1t27b4448/kJ6ejgkTJmDXrl0wMzNDSUkJoqOj8f695DOwEUKIxJQgs3M+KkZLSwtjxozB2bNncePGDcyYMQPLli2DsbEx+vbty3V4hBAFw9WUAlWJ88T+MVtbW/zyyy94+vQpdu7cyXU4hBAFRMMdOaKiogIPDw/ha7aEEFJZqnG+lphcJnZCCJEZJcjslNgJIUqlOvedS4oSOyFEqUizglJ1RYmdEKJUqvNDUUlRYieEKBnFz+yU2AkhSoVa7IQQomCUIK9TYieEKBdqsRNCiIKh4Y6EEKJoFD+vU2InhCgXJcjrlNgJIcqFrwSd7JTYCSHKRfHzunxN20sIIbImq3U2rKysyl031c/PDwDQpUsXsX0TJ06srNsSQS12QohSkVVPzKVLl1BcXCz8fPPmTfTo0QODBw8Wlo0fPx5BQUHCzzVq1JBJLJTYCSFKRVbDHY2MjEQ+L1u2DNbW1ujcubOwrEaNGjAxMZHJ9T9GXTGEEKUizQpKBQUFyMrKEtkKCgq+eo0PHz5gx44dGDNmDHgf/YkQFhaGWrVqoXHjxggICEBubq5M7pESOyGEfEZwcDD09PREtuDg4K8eFxUVhXfv3mHUqFHCsuHDh2PHjh2IjY1FQEAAtm/fDm9vb5nEzWOMMZmcmUMn777iOgRShTyXn+Q6BFKFXm4bWqHj3+UVf73S/9PkF4m10AUCAQQCwRePc3V1hbq6Og4cOPDZOjExMejWrRtSUlJgbW0tcUySoD52QohSkaaPXZIk/qlHjx7hxIkT+Pvvv79Yr02bNgBAiZ0QQipK1u8nbdu2DcbGxujVq9cX6yUmJgIATE1NKz0GSuyEEKUiy8ReUlKCbdu2wcfHB6qq/6XX+/fvIzw8HD179kTNmjVx/fp1TJs2DZ06dUKTJk0qPQ5K7IQQpSLL2R1PnDiBx48fY8yYMSLl6urqOHHiBNasWYOcnByYm5tj4MCB+Pnnn2USByV2QohSkWWL3cXFBeWNRzE3N8fp06dld+FPUGInhCgVJZgqhhI7IUTJKEFmp8ROCFEqtIISIYQoGCWYjl0x3zxVRgUFBQgODkZAQIDUL1SQ6od+3uRLKLEriKysLOjp6SEzMxO6urpch0NkjH7e5EtoEjBCCFEwlNgJIUTBUGInhBAFQ4ldQQgEAgQGBtKDNCVBP2/yJfTwlBBCFAy12AkhRMFQYieEEAVDiZ0QQhQMJXYFwePxEBUVJbPznzp1CjweD+/evZPZNZRFVXyXCxYsQNOmTWV2fiLfKLFLYNSoUeDxeFi2bJlIeVRUFHhVNPFEXl4eDA0NUatWLbHFdatC+/btkZ6eDj09vSq/dnUVHx8PFRWVry6RJgszZ87EyZO0yLeyosQuIQ0NDSxfvhxv377l5PoRERFwcHBAo0aNZNoy/xx1dXWYmJhU2S8yRbB161ZMnjwZcXFxSEtLq9Jra2tro2bNmlV6TSI/KLFLqHv37jAxMUFwcPBn65QlX4FAACsrK6xcuVJkv5WVFZYuXYoxY8ZAR0cHFhYW2LJli0TX37p1K7y9veHt7Y2tW7eWWyc9PR3u7u7Q1NRE/fr1sW/fPuG+8v78T0xMBI/Hw8OHDwGUrq7ep08fGBgYQEtLCw4ODjh8+LDY8VlZWdDU1MSRI0dErh8ZGQkdHR3k5uYCAJ48eYIhQ4ZAX18fhoaG6Nevn/Baii47Oxu7d+/GpEmT0KtXL4SEhIjVOXfuHJo0aQINDQ20bdsWN2/eFO4rrytlzZo1sLKyEn4+deoUWrduDS0tLejr68PZ2RmPHj0SO/748ePQ0NAQ6/qZOnUqvvvuO+Hns2fPomPHjtDU1IS5uTmmTJmCnJycCn0PhBuU2CWkoqKCpUuXYv369Xj69KnY/oSEBAwZMgSenp64ceMGFixYgHnz5on9B71y5Uq0bNkSV69exffff49JkyYhKSnpi9e+f/8+4uPjMWTIEAwZMgRnzpwR/gf8sXnz5mHgwIG4du0avLy84OnpiTt37kh8j35+figoKEBcXBxu3LiB5cuXQ1tbW6yerq4uevfujfDwcJHysLAweHh4oEaNGigsLISrqyt0dHRw5swZnDt3Dtra2nBzc8OHDx8kjqm62rNnDxo1agRbW1t4e3vjzz//FFsybdasWVi5ciUuXboEIyMj9OnTB4WFhRKdv6ioCB4eHujcuTOuX7+O+Ph4+Pr6lvsXVbdu3aCvr4+IiAhhWXFxMXbv3g0vLy8Apf8fc3Nzw8CBA3H9+nXs3r0bZ8+ehb+/fwW+BcIZRr7Kx8eH9evXjzHGWNu2bdmYMWMYY4xFRkaysq9w+PDhrEePHiLHzZo1i9nb2ws/W1paMm9vb+HnkpISZmxszDZu3PjF6//444/Mw8ND+Llfv34sMDBQpA4ANnHiRJGyNm3asEmTJjHGGIuNjWUA2Nu3b4X7r169ygCw1NRUxhhjjo6ObMGCBeXG8OnxkZGRTFtbm+Xk5DDGGMvMzGQaGhrsyJEjjDHGtm/fzmxtbVlJSYnwHAUFBUxTU5MdO3bsi/erCNq3b8/WrFnDGGOssLCQ1apVi8XGxjLG/vsud+3aJaz/+vVrpqmpyXbv3s0YYywwMJA5OTmJnHP16tXM0tJSWB8AO3XqVLnX//T4qVOnsu+++074+dixY0wgEAh/nmPHjmW+vr4i5zhz5gzj8/ksLy9P2tsnHKMWu5SWL1+O0NBQsZbwnTt34OzsLFLm7OyM5ORkFBcXC8uaNGki/DePx4OJiQlevHgBAHB3d4e2tja0tbXh4OAAoLRlFRoaCm9vb+Fx3t7eCAkJQUlJicj12rVrJ/ZZmhb7lClTsHjxYjg7OyMwMBDXr1//bN2ePXtCTU0N+/fvB1DaDaWrq4vu3bsDAK5du4aUlBTo6OgI78nQ0BD5+fm4f/++xDFVR0lJSbh48SKGDRsGAFBVVcXQoUPFutA+/nkZGhrC1tZW4p+XoaEhRo0aBVdXV/Tp0wdr165Fenr6Z+t7eXnh1KlTwr7+sLAw9OrVC/r6+gBKf14hISHCn5W2tjZcXV1RUlKC1NRUaW6fyAFK7FLq1KkTXF1dERAQ8E3Hq6mpiXzm8XjCBP2///0PiYmJSExMFPZtHzt2DM+ePcPQoUOhqqoKVVVVeHp64tGjR1KNeuDzS3/U7KPugE//7B83bhwePHiAESNG4MaNG2jZsiXWr19f7vnU1dUxaNAgYXdMeHi4MEagtI+5RYsWwvsp2+7du4fhw4dLHHd1tHXrVhQVFcHMzEz4M9u4cSMiIiKQmZkp0Tn4fL5Y182nP69t27YhPj4e7du3x+7du9GwYUP8+++/5Z6vVatWsLa2xq5du5CXl4fIyEhhNwxQ+vOaMGGCyM/q2rVrSE5OhrW1tZTfAOEaLY33DZYtW4amTZvC1tZWWGZnZ4dz586J1Dt37hwaNmwIFRUVic5bp04dsbKtW7fC09MTP/30k0j5kiVLsHXrVvTo0UNY9u+//2LkyJEin5s1awYAMDIyAlD6gNXAwABA6cPTT5mbm2PixImYOHEiAgIC8Mcff2Dy5Mnlxuvl5YUePXrg1q1biImJweLFi4X7mjdvjt27d8PY2FipFoIoKirCX3/9hZUrV8LFxUVkn4eHB3bu3IlGjRoBKP35WFhYAADevn2Le/fuwc7ODkDpzysjIwOMMWG/eXk/r2bNmqFZs2YICAhAu3btEB4ejrZt25Ybm5eXF8LCwlC3bl3w+XyRYZjNmzfH7du3YWNjU+HvgMgBjruCqoWP+9jLjBgxgmloaAj72BMSEhifz2dBQUEsKSmJhYSEME1NTbZt2zbhMZaWlmz16tUi53FychLrLy/z4sULpqamJuy3/tjhw4eZQCBgr1+/ZoyV9rHXqlWLbd26lSUlJbH58+czPp/Pbt26xRhj7MOHD8zc3JwNHjyY3bt3jx08eJDZ2tqK9LFPnTqVHT16lD148IAlJCSwNm3asCFDhjDGyu+jLykpYebm5szJyYlZW1uLxJeTk8MaNGjAunTpwuLi4tiDBw9YbGwsmzx5Mnvy5MmXvu5qLTIykqmrq7N3796J7Zs9ezZr2bKl8Lt0cHBgJ06cYDdu3GB9+/ZlFhYWrKCggDHG2O3btxmPx2PLli1jKSkpbMOGDczAwEDYx/7gwQM2d+5cdv78efbw4UN27NgxVrNmTfb7778zxsrvo09OTmYAWJMmTdjYsWNF9l27do1pamoyPz8/dvXqVXbv3j0WFRXF/Pz8Kv9LIjJHiV0C5SX21NRUpq6uzj7+3bhv3z5mb2/P1NTUmIWFBVuxYoXIMdIm9l9//ZXp6+uzDx8+iO0rKChg+vr6bO3atYyx0sT+22+/sR49ejCBQMCsrKyED+LKnD17ljk6OjINDQ3WsWNHtnfvXpHE7u/vz6ytrZlAIGBGRkZsxIgR7NWrV4yx8hM7Y6XJCgCbP3++WIzp6els5MiRrFatWkwgELD69euz8ePHs8zMzHLvVxH07t2b9ezZs9x9Fy5cYADY2rVrGQB24MAB5uDgwNTV1Vnr1q3ZtWvXROpv3LiRmZubMy0tLTZy5Ei2ZMkSYWLPyMhgHh4ezNTUlKmrqzNLS0s2f/58VlxczBgrP7Ezxljr1q0ZABYTEyO27+LFi6xHjx5MW1ubaWlpsSZNmrAlS5ZU7AshnKBpewkhRMHQw1NCCFEwlNgJIUTBUGInhBAFQ4mdEEIUDCV2QghRMJTYCSFEwVBiJ4QQBUOJnRBCFAwldlKtjBo1Ch4eHsLPXbp0wQ8//FDlcdAasESeUWInlaJsXVgejwd1dXXY2NggKCgIRUVFMr3u33//jUWLFklUl5IxURY0uyOpNG5ubti2bRsKCgpw+PBh+Pn5QU1NTWyK4w8fPkBdXb1SrmloaFgp5yFEkVCLnVQagUAAExMTWFpaYtKkSejevTv2798v7D5ZsmQJzMzMhNMdf21N1OLiYkyfPh36+vqoWbMmZs+eLTZH+addMQUFBZgzZw7Mzc0hEAhgY2ODrVu34uHDh+jatSsAwMDAADweD6NGjQIAlJSUIDg4GPXq1YOmpiacnJxE1osFgMOHD6Nhw4bQ1NRE165dlWbtVlI9UWInMqOpqSlc3/TkyZNISkpCdHQ0Dh48KNGaqCtXrkRISAj+/PNPnD17Fm/evEFkZOQXrzly5Ejs3LkT69atw507d7B582Zoa2vD3NxcuOZnUlIS0tPTsXbtWgBAcHAw/vrrL2zatAm3bt3CtGnT4O3tjdOnTwMo/QU0YMAA9OnTB4mJiRg3bhzmzp0rq6+NkIrjeHZJoiA+ntq4pKSERUdHM4FAwGbOnMl8fHxY7dq1hXONMybZmqimpqbsl19+Ee4vLCxkdevWFZlCuXPnzmzq1KmMMcaSkpIYABYdHV1ujOVNPZyfn89q1KjBzp8/L1J37NixbNiwYYwxxgICAkTWrmWMsTlz5pQ7jTEh8oD62EmlOXjwILS1tVFYWIiSkhIMHz4cCxYsgJ+fHxwdHUX61T9eE/VjZWuiZmZmIj09HW3atBHuU1VVRcuWLcW6Y8okJiZCRUUFnTt3ljjmlJQU5ObmiqxEBZQ+ByhbferOnTsicQDi68sSIk8osZNK07VrV2zcuBHq6urC9T7LaGlpidQtWxM1LCxM7Dxly/hJS1NTU+pjsrOzAQCHDh0SW5pQIBB8UxyEcI0SO6k0WlpaEq+ZKcmaqKamprhw4QI6deoEoHQ90YSEBDRv3rzc+o6OjigpKcHp06fRvXt3sf1lfzEUFxcLy+zt7SEQCPD48ePPtvTt7Oywf/9+kbLPLRpNiDygh6eEE15eXqhVqxb69euHM2fOIDU1FadOncKUKVPw9OlTAMDUqVOxbNkyREVF4e7du/j++++/OAbdysoKPj4+GDNmDKKiooTn3LNnDwDA0tISPB4PBw8exMuXL5GdnQ0dHR3MnDkT06ZNQ2hoKO7fv48rV65g/fr1CA0NBQBMnDgRycnJmDVrFpKSkhAeHo6QkBBZf0WEfDNK7IQTNWrUQFxcHCwsLDBgwADY2dlh7NixyM/PF7bgZ8yYgREjRsDHxwft2rWDjo4O+vfv/8Xzbty4EYMGDcL333+PRo0aYfz48cjJyQEA1KlTBwsXLsTcuXNRu3Zt+Pv7AwAWLVqEefPmITg4GHZ2dnBzc8OhQ4dQr149AICFhQUiIiIQFRUFJycnbNq0CUuXLpXht0NIxdCap4QQomCoxU4IIQqGEjshhCgYSuyEEKJgKLETQoiCocROCCEKhhI7IYQoGErshBCiYCixE0KIgqHETgghCoYSOyGEKBhK7IQQomD+D3eeKGcKbjsAAAAAAElFTkSuQmCC\n"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\nall_preds = []\nall_targets = []\nsample_texts = []  \ntest_df = test_with_label[['cleanText', 'enc_label']]\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(test_bar):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        all_targets.extend(targets.cpu().numpy())  # Collect true labels\n\n        # Collect the 'cleanText' for the current batch from the test dataframe\n        start_idx = batch_idx * BATCH_SIZE\n        end_idx = start_idx + len(batch['ids'])\n        sample_texts.extend(test_df['cleanText'].iloc[start_idx:end_idx].tolist())\n\n# Display some sample predictions\nnum_samples = 8  # Number of examples to display\nsample_indices = np.random.choice(len(all_targets), num_samples, replace=False)\n\n# Prepare the sample data for display\nsample_data = {\n    \"Text\": [sample_texts[i] for i in sample_indices],\n    \"Actual and Predicted Label\": [\n        f\"{all_targets[i]} -> {all_preds[i]}\" for i in sample_indices  # Display just 0 or 1\n    ]\n}\nsample_df = pd.DataFrame(sample_data)\n\n# Display the sample predictions\nprint(\"\\nSample Predictions:\")\nprint(sample_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T07:33:03.860559Z","iopub.execute_input":"2025-03-07T07:33:03.860897Z","iopub.status.idle":"2025-03-07T07:33:11.992199Z","shell.execute_reply.started":"2025-03-07T07:33:03.860856Z","shell.execute_reply":"2025-03-07T07:33:11.991336Z"}},"outputs":[{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 20/20 [00:08<00:00,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"\nSample Predictions:\n                                                Text  \\\n0  നിങ്ങളെ ഒരുപാട് ഇഷ്ട്ടം ആയിരുന്നു ഇപ്പോ ഫുൾ അഭ...   \n1  ഇതൊക്കെ ആരാ എഴുതി തരുന്നത് കണ്ടാ പറയില്ലട്ടാ ബ...   \n2  ഇനി വീണസ്ഥലത്ത് കിടന്ന് ഉരുളണ്ട ഞങ്ങൾ മണ്ടന്മാ...   \n3  കൈ വിട്ട ആയുധവും വാ വിട്ട വാക്കും തിരിച്ചെടുക്...   \n4           ഈ പ്രശ്നത്തിൽ ഇടപെടാൻ ഈ സൂരജ് മൈരൻ ആരാണ്   \n5  ഒരുപാട് ഇഷ്ടപ്പെട്ടിരിന്നു നിങ്ങളുടെ കോംബോ പക്...   \n6  നീ 50ലക്ഷത്തിനു അർഹയല്ല നിന്നെക്കാൾ അർഹതയുള്ളവ...   \n7  സീരിയസ് ആയിട്ട് പറഞ്ഞതാണെങ്കിൽ വൻ കോമഡി ആയിട്ട...   \n\n  Actual and Predicted Label  \n0                     0 -> 1  \n1                     1 -> 1  \n2                     0 -> 1  \n3                     0 -> 0  \n4                     0 -> 0  \n5                     0 -> 0  \n6                     1 -> 0  \n7                     1 -> 0  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"**Another iteration**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# Set a random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for the selected model\nmodel_name = 'l3cube-pune/malayalam-bert'  # Change model as needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 256\nBATCH_SIZE = 16\nsource_to_idx = {'Abusive': 1, 'Non-Abusive': 0}\n\n# Prepare datasets\ntrain_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ntest_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n\nX_train_text = train_df['cleanText'].tolist()\ny_train_labels = train_df['enc_label'].tolist()\n\nX_dev_text = dev_df['cleanText'].tolist()\ny_dev_labels = dev_df['enc_label'].tolist()\nX_test_text = test_with_label['cleanText'].tolist()\ny_test_labels = test_with_label['enc_label'].tolist()  # Ensure labels are here\n\n# Create dataset objects\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        \n        if self.is_labeled and self.labels is not None:  # Check if labels are provided\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\ntrain_set = NewsDataset(X_train_text, y_train_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = NewsDataset(X_dev_text, y_dev_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = NewsDataset(X_test_text, y_test_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)  # Pass labels here\n\n# Create data loaders\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:29:12.002455Z","iopub.execute_input":"2025-01-29T13:29:12.002773Z","iopub.status.idle":"2025-01-29T13:29:13.545211Z","shell.execute_reply.started":"2025-01-29T13:29:12.002726Z","shell.execute_reply":"2025-01-29T13:29:13.544301Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef83a3e044d340d48d3e130acadb05c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e962dd0c5e504e3eb74555b8141c147e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd4697f77d0f429aae6ce5f9c11dd6f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d018b1aa303246f8a6a97ac5c6429efe"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-18-707c84496ac0>:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-18-707c84496ac0>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-18-707c84496ac0>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  test_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from transformers import AutoModel\n# BERT Classifier Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BERTClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(0.4)\n        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize model\nmodel = BERTClassifier(model_name=model_name, num_labels=len(source_to_idx))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:29:17.363599Z","iopub.execute_input":"2025-01-29T13:29:17.363933Z","iopub.status.idle":"2025-01-29T13:29:23.150000Z","shell.execute_reply.started":"2025-01-29T13:29:17.363904Z","shell.execute_reply":"2025-01-29T13:29:23.149115Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f8a92570a5463abb52a90389c614f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/951M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64681d7becad44358db0b1d952d136bd"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertModel were not initialized from the model checkpoint at l3cube-pune/malayalam-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(197285, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.4, inplace=False)\n  (out): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Calculate class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.array([0, 1]),  # Convert classes to a NumPy array\n    y=train_df['enc_label']\n)\n\n# Convert class weights to a tensor\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:29:23.151167Z","iopub.execute_input":"2025-01-29T13:29:23.151475Z","iopub.status.idle":"2025-01-29T13:29:23.160866Z","shell.execute_reply.started":"2025-01-29T13:29:23.151440Z","shell.execute_reply":"2025-01-29T13:29:23.160123Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(params=model.parameters(), lr=3e-6, weight_decay=1e-4)\nloss_function = nn.CrossEntropyLoss(weight=class_weights)\n# Training loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    model.train()\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n\n    total_train_loss = 0\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for batch in train_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(ids, mask)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train_correct += (predicted == targets).sum().item()\n        total_train_samples += targets.size(0)\n\n        train_bar.set_postfix(loss=loss.item())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = total_train_correct / total_train_samples\n\n    model.eval()\n    total_val_loss = 0\n    total_val_correct = 0\n    total_val_samples = 0\n    dev_bar = tqdm(dev_loader, desc=f\"Epoch {epoch+1} - Validation\")\n\n    with torch.no_grad():\n        for batch in dev_bar:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(ids, mask)\n            loss = loss_function(outputs, targets)\n            total_val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_val_correct += (predicted == targets).sum().item()\n            total_val_samples += targets.size(0)\n\n            dev_bar.set_postfix(loss=loss.item())\n\n    avg_val_loss = total_val_loss / len(dev_loader)\n    val_accuracy = total_val_correct / total_val_samples\n\n    print(f\"Epoch {epoch + 1} | \"\n          f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f} | \"\n          f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n# Evaluate on the test set\nmodel.eval()  # Set the model to evaluation mode\n\nall_preds = []\nall_targets = []\n\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch in test_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        if 'targets' in batch:  # Only collect true labels if they exist\n            all_targets.extend(batch['targets'].cpu().numpy())  # Collect true labels\n\n# Generate the classification report if labels exist\nif len(all_targets) > 0:\n    report = classification_report(all_targets, all_preds, target_names=['Non-Abusive', 'Abusive'])\n    print(\"Classification Report on Test Set:\")\n    print(report)\nelse:\n    print(\"Test set does not contain labels, only predictions are available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T13:29:29.081405Z","iopub.execute_input":"2025-01-29T13:29:29.081702Z","iopub.status.idle":"2025-01-29T13:58:10.367975Z","shell.execute_reply.started":"2025-01-29T13:29:29.081679Z","shell.execute_reply":"2025-01-29T13:58:10.366978Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 - Training: 100%|██████████| 184/184 [02:08<00:00,  1.43it/s, loss=0.68] \nEpoch 1 - Validation: 100%|██████████| 40/40 [00:09<00:00,  4.43it/s, loss=0.698]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Training Loss: 0.6933, Training Accuracy: 0.5220 | Validation Loss: 0.6944, Validation Accuracy: 0.4817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.635]\nEpoch 2 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.57it/s, loss=0.668]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Training Loss: 0.6896, Training Accuracy: 0.5404 | Validation Loss: 0.6758, Validation Accuracy: 0.6979\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.615]\nEpoch 3 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.58it/s, loss=0.573]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Training Loss: 0.6564, Training Accuracy: 0.7027 | Validation Loss: 0.6225, Validation Accuracy: 0.7313\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.494]\nEpoch 4 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.56it/s, loss=0.63] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Training Loss: 0.6020, Training Accuracy: 0.7634 | Validation Loss: 0.5885, Validation Accuracy: 0.7488\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.384]\nEpoch 5 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.56it/s, loss=0.644]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Training Loss: 0.5490, Training Accuracy: 0.8026 | Validation Loss: 0.5655, Validation Accuracy: 0.7552\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.51] \nEpoch 6 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.56it/s, loss=0.573]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Training Loss: 0.4951, Training Accuracy: 0.8282 | Validation Loss: 0.5445, Validation Accuracy: 0.7599\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.518]\nEpoch 7 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.55it/s, loss=0.709]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Training Loss: 0.4512, Training Accuracy: 0.8479 | Validation Loss: 0.5397, Validation Accuracy: 0.7615\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.307]\nEpoch 8 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.56it/s, loss=0.795]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Training Loss: 0.4018, Training Accuracy: 0.8725 | Validation Loss: 0.5383, Validation Accuracy: 0.7647\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.188]\nEpoch 9 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.55it/s, loss=0.872]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Training Loss: 0.3611, Training Accuracy: 0.8899 | Validation Loss: 0.5577, Validation Accuracy: 0.7568\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.466]\nEpoch 10 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.57it/s, loss=0.921]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Training Loss: 0.3364, Training Accuracy: 0.8974 | Validation Loss: 0.5840, Validation Accuracy: 0.7536\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.23] \nEpoch 11 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.55it/s, loss=0.901]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Training Loss: 0.3179, Training Accuracy: 0.9011 | Validation Loss: 0.5706, Validation Accuracy: 0.7695\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 - Training: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s, loss=0.113]\nEpoch 12 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.57it/s, loss=0.952]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Training Loss: 0.2869, Training Accuracy: 0.9161 | Validation Loss: 0.5881, Validation Accuracy: 0.7615\n","output_type":"stream"},{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 40/40 [00:08<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"Classification Report on Test Set:\n              precision    recall  f1-score   support\n\n Non-Abusive       0.63      0.84      0.72       306\n     Abusive       0.78      0.54      0.64       323\n\n    accuracy                           0.69       629\n   macro avg       0.71      0.69      0.68       629\nweighted avg       0.71      0.69      0.68       629\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Evaluate on the test set\nmodel.eval()  # Set the model to evaluation mode\n\nall_preds = []\nall_targets = []\n\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch in test_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        if 'targets' in batch:  # Only collect true labels if they exist\n            all_targets.extend(batch['targets'].cpu().numpy())  # Collect true labels\n\n# Generate the classification report if labels exist\nif len(all_targets) > 0:\n    # Print classification report\n    report = classification_report(all_targets, all_preds, target_names=['Non-Abusive', 'Abusive'])\n    print(\"Classification Report on Test Set:\")\n    print(report)\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(all_targets, all_preds)\n\n    # Plot the confusion matrix as a heatmap\n    plt.figure(figsize=(4, 3))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Abusive', 'Abusive'], yticklabels=['Non-Abusive', 'Abusive'])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n\nelse:\n    print(\"Test set does not contain labels, only predictions are available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T14:05:17.984358Z","iopub.execute_input":"2025-01-29T14:05:17.984681Z","iopub.status.idle":"2025-01-29T14:05:26.967985Z","shell.execute_reply.started":"2025-01-29T14:05:17.984657Z","shell.execute_reply":"2025-01-29T14:05:26.967239Z"}},"outputs":[{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 40/40 [00:08<00:00,  4.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classification Report on Test Set:\n              precision    recall  f1-score   support\n\n Non-Abusive       0.63      0.84      0.72       306\n     Abusive       0.78      0.54      0.64       323\n\n    accuracy                           0.69       629\n   macro avg       0.71      0.69      0.68       629\nweighted avg       0.71      0.69      0.68       629\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 400x300 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"**xlm-roberta-base**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# Set a random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for the selected model\nmodel_name = 'FacebookAI/xlm-roberta-base'  # Change model as needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 256\nBATCH_SIZE = 32\nsource_to_idx = {'Abusive': 1, 'Non-Abusive': 0}\n\n# Prepare datasets\ntrain_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ntest_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n\nX_train_text = train_df['cleanText'].tolist()\ny_train_labels = train_df['enc_label'].tolist()\n\nX_dev_text = dev_df['cleanText'].tolist()\ny_dev_labels = dev_df['enc_label'].tolist()\nX_test_text = test_with_label['cleanText'].tolist()\ny_test_labels = test_with_label['enc_label'].tolist()  # Ensure labels are here\n\n# Create dataset objects\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        \n        if self.is_labeled and self.labels is not None:  # Check if labels are provided\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\ntrain_set = NewsDataset(X_train_text, y_train_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = NewsDataset(X_dev_text, y_dev_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = NewsDataset(X_test_text, y_test_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)  # Pass labels here\n\n# Create data loaders\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T06:14:14.105460Z","iopub.execute_input":"2025-01-29T06:14:14.105798Z","iopub.status.idle":"2025-01-29T06:14:17.854007Z","shell.execute_reply.started":"2025-01-29T06:14:14.105773Z","shell.execute_reply":"2025-01-29T06:14:17.853085Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a277851f53d47dabff529eeb0baaf72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c7d3fcc257434b933748f483f0e9e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbdb3bc431a44db6ab098a8c4d516b73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9c1ac0682e745788baa197f28db00d9"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-14-7e7b8befda10>:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-14-7e7b8befda10>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-14-7e7b8befda10>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  test_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from transformers import AutoModel\n# BERT Classifier Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BERTClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(0.5)\n        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize model\nmodel = BERTClassifier(model_name=model_name, num_labels=len(source_to_idx))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T06:14:20.461941Z","iopub.execute_input":"2025-01-29T06:14:20.462235Z","iopub.status.idle":"2025-01-29T06:14:41.094052Z","shell.execute_reply.started":"2025-01-29T06:14:20.462214Z","shell.execute_reply":"2025-01-29T06:14:41.093269Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d255508861e465791d548b62887550f"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): XLMRobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.5, inplace=False)\n  (out): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Calculate class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.array([0, 1]),  # Convert classes to a NumPy array\n    y=train_df['enc_label']\n)\n\n# Convert class weights to a tensor\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T06:14:41.095145Z","iopub.execute_input":"2025-01-29T06:14:41.095466Z","iopub.status.idle":"2025-01-29T06:14:41.109566Z","shell.execute_reply.started":"2025-01-29T06:14:41.095433Z","shell.execute_reply":"2025-01-29T06:14:41.108937Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(params=model.parameters(), lr=2e-6, weight_decay=1e-4)\nloss_function = nn.CrossEntropyLoss(weight=class_weights)\n# Training loop\nEPOCHS = 15\nfor epoch in range(EPOCHS):\n    model.train()\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n\n    total_train_loss = 0\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for batch in train_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(ids, mask)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train_correct += (predicted == targets).sum().item()\n        total_train_samples += targets.size(0)\n\n        train_bar.set_postfix(loss=loss.item())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = total_train_correct / total_train_samples\n\n    model.eval()\n    total_val_loss = 0\n    total_val_correct = 0\n    total_val_samples = 0\n    dev_bar = tqdm(dev_loader, desc=f\"Epoch {epoch+1} - Validation\")\n\n    with torch.no_grad():\n        for batch in dev_bar:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(ids, mask)\n            loss = loss_function(outputs, targets)\n            total_val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_val_correct += (predicted == targets).sum().item()\n            total_val_samples += targets.size(0)\n\n            dev_bar.set_postfix(loss=loss.item())\n\n    avg_val_loss = total_val_loss / len(dev_loader)\n    val_accuracy = total_val_correct / total_val_samples\n\n    print(f\"Epoch {epoch + 1} | \"\n          f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f} | \"\n          f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n# Evaluate on the test set\nmodel.eval()  # Set the model to evaluation mode\n\nall_preds = []\nall_targets = []\n\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch in test_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        if 'targets' in batch:  # Only collect true labels if they exist\n            all_targets.extend(batch['targets'].cpu().numpy())  # Collect true labels\n\n# Generate the classification report if labels exist\nif len(all_targets) > 0:\n    report = classification_report(all_targets, all_preds, target_names=['Non-Abusive', 'Abusive'])\n    print(\"Classification Report on Test Set:\")\n    print(report)\nelse:\n    print(\"Test set does not contain labels, only predictions are available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T06:14:41.110692Z","iopub.execute_input":"2025-01-29T06:14:41.110912Z","iopub.status.idle":"2025-01-29T06:48:44.089521Z","shell.execute_reply.started":"2025-01-29T06:14:41.110865Z","shell.execute_reply":"2025-01-29T06:48:44.088675Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 - Training: 100%|██████████| 92/92 [01:58<00:00,  1.29s/it, loss=0.684]\nEpoch 1 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.36it/s, loss=0.687]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Training Loss: 0.7170, Training Accuracy: 0.4944 | Validation Loss: 0.6901, Validation Accuracy: 0.5183\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Training: 100%|██████████| 92/92 [02:08<00:00,  1.39s/it, loss=0.679]\nEpoch 2 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s, loss=0.678]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Training Loss: 0.7054, Training Accuracy: 0.4937 | Validation Loss: 0.6839, Validation Accuracy: 0.5342\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Training: 100%|██████████| 92/92 [02:08<00:00,  1.39s/it, loss=0.692]\nEpoch 3 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s, loss=0.653]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Training Loss: 0.6984, Training Accuracy: 0.5309 | Validation Loss: 0.6707, Validation Accuracy: 0.5819\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Training: 100%|██████████| 92/92 [02:08<00:00,  1.39s/it, loss=0.619]\nEpoch 4 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s, loss=0.631]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Training Loss: 0.6844, Training Accuracy: 0.5673 | Validation Loss: 0.6422, Validation Accuracy: 0.6582\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.39s/it, loss=0.718]\nEpoch 5 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s, loss=0.624]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Training Loss: 0.6779, Training Accuracy: 0.5714 | Validation Loss: 0.6294, Validation Accuracy: 0.6789\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.39s/it, loss=0.796]\nEpoch 6 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s, loss=0.646]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Training Loss: 0.6677, Training Accuracy: 0.5885 | Validation Loss: 0.6319, Validation Accuracy: 0.6598\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Training: 100%|██████████| 92/92 [02:08<00:00,  1.39s/it, loss=0.711]\nEpoch 7 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s, loss=0.641]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Training Loss: 0.6704, Training Accuracy: 0.5806 | Validation Loss: 0.6420, Validation Accuracy: 0.6391\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Training: 100%|██████████| 92/92 [02:08<00:00,  1.39s/it, loss=0.731]\nEpoch 8 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s, loss=0.668]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Training Loss: 0.6788, Training Accuracy: 0.5827 | Validation Loss: 0.6493, Validation Accuracy: 0.6169\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.39s/it, loss=0.631]\nEpoch 9 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s, loss=0.618]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Training Loss: 0.6705, Training Accuracy: 0.5932 | Validation Loss: 0.6239, Validation Accuracy: 0.6852\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Training: 100%|██████████| 92/92 [02:08<00:00,  1.39s/it, loss=0.573]\nEpoch 10 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s, loss=0.598]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Training Loss: 0.6457, Training Accuracy: 0.6365 | Validation Loss: 0.6018, Validation Accuracy: 0.7027\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.39s/it, loss=0.759]\nEpoch 11 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s, loss=0.596]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Training Loss: 0.6350, Training Accuracy: 0.6526 | Validation Loss: 0.6023, Validation Accuracy: 0.6932\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.39s/it, loss=0.669]\nEpoch 12 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.40it/s, loss=0.6]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Training Loss: 0.6320, Training Accuracy: 0.6502 | Validation Loss: 0.5878, Validation Accuracy: 0.7043\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 - Training: 100%|██████████| 92/92 [02:08<00:00,  1.39s/it, loss=0.611]\nEpoch 13 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s, loss=0.581]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Training Loss: 0.6175, Training Accuracy: 0.6580 | Validation Loss: 0.5805, Validation Accuracy: 0.7122\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.39s/it, loss=0.647]\nEpoch 14 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s, loss=0.628]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Training Loss: 0.6038, Training Accuracy: 0.6723 | Validation Loss: 0.5887, Validation Accuracy: 0.7138\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.39s/it, loss=0.529]\nEpoch 15 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.40it/s, loss=0.606]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Training Loss: 0.5992, Training Accuracy: 0.6775 | Validation Loss: 0.5870, Validation Accuracy: 0.7043\n","output_type":"stream"},{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s]","output_type":"stream"},{"name":"stdout","text":"Classification Report on Test Set:\n              precision    recall  f1-score   support\n\n Non-Abusive       0.60      0.81      0.69       306\n     Abusive       0.73      0.49      0.59       323\n\n    accuracy                           0.65       629\n   macro avg       0.67      0.65      0.64       629\nweighted avg       0.67      0.65      0.64       629\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"**bert-base-multilingual-cased**\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# Set a random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for the selected model\nmodel_name = 'google-bert/bert-base-multilingual-cased'  # Change model as needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 256\nBATCH_SIZE = 16\nsource_to_idx = {'Abusive': 1, 'Non-Abusive': 0}\n\n# Prepare datasets\ntrain_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ntest_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n\nX_train_text = train_df['cleanText'].tolist()\ny_train_labels = train_df['enc_label'].tolist()\n\nX_dev_text = dev_df['cleanText'].tolist()\ny_dev_labels = dev_df['enc_label'].tolist()\nX_test_text = test_with_label['cleanText'].tolist()\ny_test_labels = test_with_label['enc_label'].tolist()  # Ensure labels are here\n\n# Create dataset objects\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        \n        if self.is_labeled and self.labels is not None:  # Check if labels are provided\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\ntrain_set = NewsDataset(X_train_text, y_train_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = NewsDataset(X_dev_text, y_dev_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = NewsDataset(X_test_text, y_test_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)  # Pass labels here\n\n# Create data loaders\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T06:49:44.971739Z","iopub.execute_input":"2025-01-29T06:49:44.972078Z","iopub.status.idle":"2025-01-29T06:49:45.395726Z","shell.execute_reply.started":"2025-01-29T06:49:44.972049Z","shell.execute_reply":"2025-01-29T06:49:45.394794Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-32-7d4b9b69277f>:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-32-7d4b9b69277f>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-32-7d4b9b69277f>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  test_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from transformers import AutoModel\n# BERT Classifier Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BERTClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(0.4)\n        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize model\nmodel = BERTClassifier(model_name=model_name, num_labels=len(source_to_idx))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T06:49:47.610271Z","iopub.execute_input":"2025-01-29T06:49:47.610586Z","iopub.status.idle":"2025-01-29T06:49:55.106721Z","shell.execute_reply.started":"2025-01-29T06:49:47.610562Z","shell.execute_reply":"2025-01-29T06:49:55.105913Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf9c9d7c20540c391637f3242361643"}},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.4, inplace=False)\n  (out): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Calculate class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.array([0, 1]),  # Convert classes to a NumPy array\n    y=train_df['enc_label']\n)\n\n# Convert class weights to a tensor\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(params=model.parameters(), lr=3e-6, weight_decay=1e-5)\nloss_function = nn.CrossEntropyLoss(weight=class_weights)\n# Training loop\nEPOCHS = 15\nfor epoch in range(EPOCHS):\n    model.train()\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n\n    total_train_loss = 0\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for batch in train_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(ids, mask)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train_correct += (predicted == targets).sum().item()\n        total_train_samples += targets.size(0)\n\n        train_bar.set_postfix(loss=loss.item())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = total_train_correct / total_train_samples\n\n    model.eval()\n    total_val_loss = 0\n    total_val_correct = 0\n    total_val_samples = 0\n    dev_bar = tqdm(dev_loader, desc=f\"Epoch {epoch+1} - Validation\")\n\n    with torch.no_grad():\n        for batch in dev_bar:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(ids, mask)\n            loss = loss_function(outputs, targets)\n            total_val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_val_correct += (predicted == targets).sum().item()\n            total_val_samples += targets.size(0)\n\n            dev_bar.set_postfix(loss=loss.item())\n\n    avg_val_loss = total_val_loss / len(dev_loader)\n    val_accuracy = total_val_correct / total_val_samples\n\n    print(f\"Epoch {epoch + 1} | \"\n          f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f} | \"\n          f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n# Evaluate on the test set\nmodel.eval()  # Set the model to evaluation mode\n\nall_preds = []\nall_targets = []\n\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch in test_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        if 'targets' in batch:  # Only collect true labels if they exist\n            all_targets.extend(batch['targets'].cpu().numpy())  # Collect true labels\n\n# Generate the classification report if labels exist\nif len(all_targets) > 0:\n    report = classification_report(all_targets, all_preds, target_names=['Non-Abusive', 'Abusive'])\n    print(\"Classification Report on Test Set:\")\n    print(report)\nelse:\n    print(\"Test set does not contain labels, only predictions are available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T06:50:47.673912Z","iopub.execute_input":"2025-01-29T06:50:47.674253Z","iopub.status.idle":"2025-01-29T07:25:48.220652Z","shell.execute_reply.started":"2025-01-29T06:50:47.674225Z","shell.execute_reply":"2025-01-29T07:25:48.219793Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 - Training: 100%|██████████| 184/184 [02:11<00:00,  1.40it/s, loss=0.648]\nEpoch 1 - Validation: 100%|██████████| 40/40 [00:09<00:00,  4.43it/s, loss=0.601]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Training Loss: 0.6916, Training Accuracy: 0.5346 | Validation Loss: 0.6488, Validation Accuracy: 0.6264\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.65] \nEpoch 2 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.45it/s, loss=0.599]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Training Loss: 0.6610, Training Accuracy: 0.6035 | Validation Loss: 0.6273, Validation Accuracy: 0.6757\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.51] \nEpoch 3 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.45it/s, loss=0.573]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Training Loss: 0.6146, Training Accuracy: 0.6628 | Validation Loss: 0.5986, Validation Accuracy: 0.6773\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.702]\nEpoch 4 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.46it/s, loss=0.43] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Training Loss: 0.5604, Training Accuracy: 0.7119 | Validation Loss: 0.6000, Validation Accuracy: 0.6773\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.191]\nEpoch 5 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.47it/s, loss=0.375]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Training Loss: 0.5113, Training Accuracy: 0.7535 | Validation Loss: 0.6340, Validation Accuracy: 0.6900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.56] \nEpoch 6 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.48it/s, loss=0.311]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Training Loss: 0.4595, Training Accuracy: 0.7924 | Validation Loss: 0.6402, Validation Accuracy: 0.6979\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.371]\nEpoch 7 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.46it/s, loss=0.347]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Training Loss: 0.3906, Training Accuracy: 0.8323 | Validation Loss: 0.6858, Validation Accuracy: 0.6868\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.147]\nEpoch 8 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.47it/s, loss=0.485]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Training Loss: 0.3410, Training Accuracy: 0.8609 | Validation Loss: 0.7680, Validation Accuracy: 0.6900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.154] \nEpoch 9 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.46it/s, loss=0.467]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Training Loss: 0.2980, Training Accuracy: 0.8776 | Validation Loss: 0.7438, Validation Accuracy: 0.6868\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.0653]\nEpoch 10 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.48it/s, loss=0.267]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Training Loss: 0.2494, Training Accuracy: 0.9028 | Validation Loss: 0.8317, Validation Accuracy: 0.6900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.211] \nEpoch 11 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.47it/s, loss=0.666]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Training Loss: 0.2077, Training Accuracy: 0.9236 | Validation Loss: 0.8678, Validation Accuracy: 0.6900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.0637]\nEpoch 12 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.46it/s, loss=0.59] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Training Loss: 0.1764, Training Accuracy: 0.9403 | Validation Loss: 0.9066, Validation Accuracy: 0.6900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.141] \nEpoch 13 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.45it/s, loss=0.43] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Training Loss: 0.1597, Training Accuracy: 0.9407 | Validation Loss: 0.9771, Validation Accuracy: 0.6900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=1.12]  \nEpoch 14 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.45it/s, loss=0.303]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Training Loss: 0.1470, Training Accuracy: 0.9506 | Validation Loss: 1.0042, Validation Accuracy: 0.6932\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15 - Training: 100%|██████████| 184/184 [02:10<00:00,  1.41it/s, loss=0.24]  \nEpoch 15 - Validation: 100%|██████████| 40/40 [00:08<00:00,  4.46it/s, loss=0.229]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Training Loss: 0.1133, Training Accuracy: 0.9659 | Validation Loss: 1.0186, Validation Accuracy: 0.6836\n","output_type":"stream"},{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 40/40 [00:09<00:00,  4.28it/s]","output_type":"stream"},{"name":"stdout","text":"Classification Report on Test Set:\n              precision    recall  f1-score   support\n\n Non-Abusive       0.63      0.76      0.69       306\n     Abusive       0.72      0.58      0.64       323\n\n    accuracy                           0.67       629\n   macro avg       0.68      0.67      0.67       629\nweighted avg       0.68      0.67      0.67       629\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"**Another iteration**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# Set a random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for the selected model\nmodel_name = 'google-bert/bert-base-multilingual-cased'  # Change model as needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 256\nBATCH_SIZE = 32\nsource_to_idx = {'Abusive': 1, 'Non-Abusive': 0}\n\n# Prepare datasets\ntrain_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ntest_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n\nX_train_text = train_df['cleanText'].tolist()\ny_train_labels = train_df['enc_label'].tolist()\n\nX_dev_text = dev_df['cleanText'].tolist()\ny_dev_labels = dev_df['enc_label'].tolist()\nX_test_text = test_with_label['cleanText'].tolist()\ny_test_labels = test_with_label['enc_label'].tolist()  # Ensure labels are here\n\n# Create dataset objects\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        \n        if self.is_labeled and self.labels is not None:  # Check if labels are provided\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\ntrain_set = NewsDataset(X_train_text, y_train_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = NewsDataset(X_dev_text, y_dev_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = NewsDataset(X_test_text, y_test_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)  # Pass labels here\n\n# Create data loaders\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T14:05:52.743939Z","iopub.execute_input":"2025-01-29T14:05:52.744608Z","iopub.status.idle":"2025-01-29T14:05:54.011275Z","shell.execute_reply.started":"2025-01-29T14:05:52.744577Z","shell.execute_reply":"2025-01-29T14:05:54.010278Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ebb4ef6cace441399b7d9127a6bf57b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f532c48a2abb47d3ba166e2e4d8609d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e20ebbf75bbb4afca3e395678d28144c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f731c832a2045bab18f4b12d1657143"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-23-5959eca345a6>:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-23-5959eca345a6>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-23-5959eca345a6>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  test_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from transformers import AutoModel\n# BERT Classifier Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BERTClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(0.5)\n        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize model\nmodel = BERTClassifier(model_name=model_name, num_labels=len(source_to_idx))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T14:06:00.897184Z","iopub.execute_input":"2025-01-29T14:06:00.897513Z","iopub.status.idle":"2025-01-29T14:06:04.648904Z","shell.execute_reply.started":"2025-01-29T14:06:00.897483Z","shell.execute_reply":"2025-01-29T14:06:04.648182Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aa01f4f93974d15a28c53660e9a0a4b"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.5, inplace=False)\n  (out): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Calculate class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.array([0, 1]),  # Convert classes to a NumPy array\n    y=train_df['enc_label']\n)\n\n# Convert class weights to a tensor\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T14:06:04.649853Z","iopub.execute_input":"2025-01-29T14:06:04.650143Z","iopub.status.idle":"2025-01-29T14:06:04.655624Z","shell.execute_reply.started":"2025-01-29T14:06:04.650121Z","shell.execute_reply":"2025-01-29T14:06:04.654936Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(params=model.parameters(), lr=2e-6, weight_decay=1e-6)\nloss_function = nn.CrossEntropyLoss(weight=class_weights)\n# Training loop\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    model.train()\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n\n    total_train_loss = 0\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for batch in train_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(ids, mask)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train_correct += (predicted == targets).sum().item()\n        total_train_samples += targets.size(0)\n\n        train_bar.set_postfix(loss=loss.item())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = total_train_correct / total_train_samples\n\n    model.eval()\n    total_val_loss = 0\n    total_val_correct = 0\n    total_val_samples = 0\n    dev_bar = tqdm(dev_loader, desc=f\"Epoch {epoch+1} - Validation\")\n\n    with torch.no_grad():\n        for batch in dev_bar:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(ids, mask)\n            loss = loss_function(outputs, targets)\n            total_val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_val_correct += (predicted == targets).sum().item()\n            total_val_samples += targets.size(0)\n\n            dev_bar.set_postfix(loss=loss.item())\n\n    avg_val_loss = total_val_loss / len(dev_loader)\n    val_accuracy = total_val_correct / total_val_samples\n\n    print(f\"Epoch {epoch + 1} | \"\n          f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f} | \"\n          f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n# Evaluate on the test set\nmodel.eval()  # Set the model to evaluation mode\n\nall_preds = []\nall_targets = []\n\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch in test_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        if 'targets' in batch:  # Only collect true labels if they exist\n            all_targets.extend(batch['targets'].cpu().numpy())  # Collect true labels\n\n# Generate the classification report if labels exist\nif len(all_targets) > 0:\n    report = classification_report(all_targets, all_preds, target_names=['Non-Abusive', 'Abusive'])\n    print(\"Classification Report on Test Set:\")\n    print(report)\nelse:\n    print(\"Test set does not contain labels, only predictions are available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T14:06:05.210860Z","iopub.execute_input":"2025-01-29T14:06:05.211119Z","iopub.status.idle":"2025-01-29T14:28:57.890201Z","shell.execute_reply.started":"2025-01-29T14:06:05.211097Z","shell.execute_reply":"2025-01-29T14:28:57.889352Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.39s/it, loss=0.726]\nEpoch 1 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.14it/s, loss=0.671]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Training Loss: 0.6957, Training Accuracy: 0.5012 | Validation Loss: 0.6737, Validation Accuracy: 0.6312\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.38s/it, loss=0.717]\nEpoch 2 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.15it/s, loss=0.632]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Training Loss: 0.6779, Training Accuracy: 0.5677 | Validation Loss: 0.6294, Validation Accuracy: 0.6630\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.38s/it, loss=0.547]\nEpoch 3 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.16it/s, loss=0.611]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Training Loss: 0.6557, Training Accuracy: 0.6100 | Validation Loss: 0.6105, Validation Accuracy: 0.6502\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Training: 100%|██████████| 92/92 [02:06<00:00,  1.38s/it, loss=0.568]\nEpoch 4 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.16it/s, loss=0.6]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Training Loss: 0.6240, Training Accuracy: 0.6546 | Validation Loss: 0.5910, Validation Accuracy: 0.6677\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.38s/it, loss=0.584]\nEpoch 5 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.15it/s, loss=0.601]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Training Loss: 0.5837, Training Accuracy: 0.6918 | Validation Loss: 0.5838, Validation Accuracy: 0.6804\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Training: 100%|██████████| 92/92 [02:06<00:00,  1.38s/it, loss=0.566]\nEpoch 6 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.15it/s, loss=0.606]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Training Loss: 0.5434, Training Accuracy: 0.7361 | Validation Loss: 0.5874, Validation Accuracy: 0.6916\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Training: 100%|██████████| 92/92 [02:06<00:00,  1.38s/it, loss=0.459]\nEpoch 7 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.16it/s, loss=0.647]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Training Loss: 0.5190, Training Accuracy: 0.7422 | Validation Loss: 0.5952, Validation Accuracy: 0.6963\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.38s/it, loss=0.459]\nEpoch 8 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.16it/s, loss=0.608]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Training Loss: 0.4797, Training Accuracy: 0.7780 | Validation Loss: 0.6234, Validation Accuracy: 0.6900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Training: 100%|██████████| 92/92 [02:06<00:00,  1.38s/it, loss=0.509]\nEpoch 9 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.15it/s, loss=0.658]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Training Loss: 0.4538, Training Accuracy: 0.8012 | Validation Loss: 0.6146, Validation Accuracy: 0.6995\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Training: 100%|██████████| 92/92 [02:07<00:00,  1.38s/it, loss=0.602]\nEpoch 10 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.16it/s, loss=0.714]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Training Loss: 0.4147, Training Accuracy: 0.8179 | Validation Loss: 0.6709, Validation Accuracy: 0.6963\n","output_type":"stream"},{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 20/20 [00:09<00:00,  2.14it/s]","output_type":"stream"},{"name":"stdout","text":"Classification Report on Test Set:\n              precision    recall  f1-score   support\n\n Non-Abusive       0.60      0.84      0.70       306\n     Abusive       0.75      0.47      0.58       323\n\n    accuracy                           0.65       629\n   macro avg       0.68      0.66      0.64       629\nweighted avg       0.68      0.65      0.64       629\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"**Indic-bert**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# Set a random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for the selected model\nmodel_name = 'ai4bharat/indic-bert'  # Change model as needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 256\nBATCH_SIZE = 32\nsource_to_idx = {'Abusive': 1, 'Non-Abusive': 0}\n\n# Prepare datasets\ntrain_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\ntest_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n\nX_train_text = train_df['cleanText'].tolist()\ny_train_labels = train_df['enc_label'].tolist()\n\nX_dev_text = dev_df['cleanText'].tolist()\ny_dev_labels = dev_df['enc_label'].tolist()\nX_test_text = test_with_label['cleanText'].tolist()\ny_test_labels = test_with_label['enc_label'].tolist()  # Ensure labels are here\n\n# Create dataset objects\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        \n        if self.is_labeled and self.labels is not None:  # Check if labels are provided\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\ntrain_set = NewsDataset(X_train_text, y_train_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = NewsDataset(X_dev_text, y_dev_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = NewsDataset(X_test_text, y_test_labels, tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)  # Pass labels here\n\n# Create data loaders\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T15:09:41.619339Z","iopub.execute_input":"2025-03-06T15:09:41.619673Z","iopub.status.idle":"2025-03-06T15:09:45.223584Z","shell.execute_reply.started":"2025-03-06T15:09:41.619646Z","shell.execute_reply":"2025-03-06T15:09:45.222562Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0559b339941e4969b8859592b21a681a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bab5d6a54dd3469fb418c8e2d0f47caf"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-15-faa48951d4fb>:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_df['enc_label'] = train_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-15-faa48951d4fb>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dev_df['enc_label'] = dev_df['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')\n<ipython-input-15-faa48951d4fb>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  test_with_label['enc_label'] = test_with_label['Class'].replace({'Abusive': 1, 'Non-Abusive': 0}).astype('int64')  # Fix here\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import AutoModel\n# BERT Classifier Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BERTClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(0.5)\n        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize model\nmodel = BERTClassifier(model_name=model_name, num_labels=len(source_to_idx))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T15:09:57.366529Z","iopub.execute_input":"2025-03-06T15:09:57.366827Z","iopub.status.idle":"2025-03-06T15:09:59.897278Z","shell.execute_reply.started":"2025-03-06T15:09:57.366805Z","shell.execute_reply":"2025-03-06T15:09:59.896000Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed115920dfa142c094bed242260896e3"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): AlbertModel(\n    (embeddings): AlbertEmbeddings(\n      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (encoder): AlbertTransformer(\n      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n      (albert_layer_groups): ModuleList(\n        (0): AlbertLayerGroup(\n          (albert_layers): ModuleList(\n            (0): AlbertLayer(\n              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (attention): AlbertSdpaAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (attention_dropout): Dropout(p=0, inplace=False)\n                (output_dropout): Dropout(p=0, inplace=False)\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              )\n              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n              (activation): GELUActivation()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (pooler): Linear(in_features=768, out_features=768, bias=True)\n    (pooler_activation): Tanh()\n  )\n  (drop): Dropout(p=0.5, inplace=False)\n  (out): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Calculate class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.array([0, 1]),  # Convert classes to a NumPy array\n    y=train_df['enc_label']\n)\n\n# Convert class weights to a tensor\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T15:10:04.263759Z","iopub.execute_input":"2025-03-06T15:10:04.264073Z","iopub.status.idle":"2025-03-06T15:10:04.277088Z","shell.execute_reply.started":"2025-03-06T15:10:04.264047Z","shell.execute_reply":"2025-03-06T15:10:04.276494Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(params=model.parameters(), lr=2e-6, weight_decay=1e-6)\nloss_function = nn.CrossEntropyLoss(weight=class_weights)\n# Training loop\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    model.train()\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n\n    total_train_loss = 0\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for batch in train_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(ids, mask)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train_correct += (predicted == targets).sum().item()\n        total_train_samples += targets.size(0)\n\n        train_bar.set_postfix(loss=loss.item())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = total_train_correct / total_train_samples\n\n    model.eval()\n    total_val_loss = 0\n    total_val_correct = 0\n    total_val_samples = 0\n    dev_bar = tqdm(dev_loader, desc=f\"Epoch {epoch+1} - Validation\")\n\n    with torch.no_grad():\n        for batch in dev_bar:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(ids, mask)\n            loss = loss_function(outputs, targets)\n            total_val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_val_correct += (predicted == targets).sum().item()\n            total_val_samples += targets.size(0)\n\n            dev_bar.set_postfix(loss=loss.item())\n\n    avg_val_loss = total_val_loss / len(dev_loader)\n    val_accuracy = total_val_correct / total_val_samples\n\n    print(f\"Epoch {epoch + 1} | \"\n          f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f} | \"\n          f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n# Evaluate on the test set\nmodel.eval()  # Set the model to evaluation mode\n\nall_preds = []\nall_targets = []\n\ntest_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n\nwith torch.no_grad():\n    for batch in test_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        outputs = model(ids, mask)\n        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n        if 'targets' in batch:  # Only collect true labels if they exist\n            all_targets.extend(batch['targets'].cpu().numpy())  # Collect true labels\n\n# Generate the classification report if labels exist\nif len(all_targets) > 0:\n    report = classification_report(all_targets, all_preds, target_names=['Non-Abusive', 'Abusive'])\n    print(\"Classification Report on Test Set:\")\n    print(report)\nelse:\n    print(\"Test set does not contain labels, only predictions are available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T15:10:38.377639Z","iopub.execute_input":"2025-03-06T15:10:38.377966Z","iopub.status.idle":"2025-03-06T15:32:37.072174Z","shell.execute_reply.started":"2025-03-06T15:10:38.377941Z","shell.execute_reply":"2025-03-06T15:32:37.071144Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 - Training: 100%|██████████| 92/92 [01:51<00:00,  1.21s/it, loss=0.702]\nEpoch 1 - Validation: 100%|██████████| 20/20 [00:08<00:00,  2.28it/s, loss=0.693]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Training Loss: 0.6932, Training Accuracy: 0.5073 | Validation Loss: 0.6935, Validation Accuracy: 0.4817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Training: 100%|██████████| 92/92 [02:02<00:00,  1.34s/it, loss=0.689]\nEpoch 2 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.22it/s, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Training Loss: 0.6930, Training Accuracy: 0.5080 | Validation Loss: 0.6930, Validation Accuracy: 0.4976\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Training: 100%|██████████| 92/92 [02:02<00:00,  1.34s/it, loss=0.682]\nEpoch 3 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.22it/s, loss=0.688]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Training Loss: 0.6927, Training Accuracy: 0.5063 | Validation Loss: 0.6910, Validation Accuracy: 0.5978\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.691]\nEpoch 4 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.22it/s, loss=0.68] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Training Loss: 0.6907, Training Accuracy: 0.5377 | Validation Loss: 0.6880, Validation Accuracy: 0.6025\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.679]\nEpoch 5 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.21it/s, loss=0.653]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Training Loss: 0.6802, Training Accuracy: 0.6086 | Validation Loss: 0.6766, Validation Accuracy: 0.6057\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.676]\nEpoch 6 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.20it/s, loss=0.616]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Training Loss: 0.6616, Training Accuracy: 0.6505 | Validation Loss: 0.6663, Validation Accuracy: 0.6121\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.583]\nEpoch 7 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.22it/s, loss=0.622]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Training Loss: 0.6366, Training Accuracy: 0.6846 | Validation Loss: 0.6621, Validation Accuracy: 0.6105\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.682]\nEpoch 8 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.21it/s, loss=0.59] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Training Loss: 0.6137, Training Accuracy: 0.7153 | Validation Loss: 0.6691, Validation Accuracy: 0.6169\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.623]\nEpoch 9 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.22it/s, loss=0.617]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Training Loss: 0.5847, Training Accuracy: 0.7463 | Validation Loss: 0.6725, Validation Accuracy: 0.6264\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Training: 100%|██████████| 92/92 [02:03<00:00,  1.34s/it, loss=0.709]\nEpoch 10 - Validation: 100%|██████████| 20/20 [00:09<00:00,  2.22it/s, loss=0.702]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Training Loss: 0.5603, Training Accuracy: 0.7678 | Validation Loss: 0.7226, Validation Accuracy: 0.6041\n","output_type":"stream"},{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 20/20 [00:08<00:00,  2.22it/s]","output_type":"stream"},{"name":"stdout","text":"Classification Report on Test Set:\n              precision    recall  f1-score   support\n\n Non-Abusive       0.55      0.70      0.62       306\n     Abusive       0.62      0.46      0.53       323\n\n    accuracy                           0.58       629\n   macro avg       0.59      0.58      0.57       629\nweighted avg       0.59      0.58      0.57       629\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18}]}